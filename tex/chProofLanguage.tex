\chapter{A proof language for formal proofs}{}
\label{ch:script}


Since proofs are just terms one could, in principle, use no proof
language and directly input proof terms instead.  Indeed this was the
modus operandi in the pioneering work of De Bruijn on Automath
(automating mathematics) in the seventies~\cite{nederpelt-94}.

Still, the use of a dedicated proof language enables a higher level
description of the formal proof being constructed.  
The \mcbSSR{} proof language give us tools to structure proofs and
to tame bookkeeping, a form of bureaucracy typical of formal proofs.

Structure is given by splitting large proofs in blocks with a
specific, declared, purpose and by factoring repetition such as
symmetric or less general cases. The main tool logic gives us is the
cut rule where the author of a proof identifies an intermediate fact,
or a generalization of the goal. The \C{have} and \C{without loss} tactics
precisely cover this need.

Bookkeeping is the ubiquitous activity of context management, that is
naming or discarding assumptions in order to track their lifetime to tide
the context up, as well as massaging assumptions and goals as to
please the formality requirements of Coq.  A paradigmatic bookkeeping
example is destructing assumption of \C{(A && B)} in order to give
distinct names to \C{A} and \C{B}: unlikely to be a pregnant step of
the proof.  The language of intro-patterns together with then
goal-as-stack model provides a very flexible tool to
succinctly deal with bookkeeping.

Structure and manageable size are key to the maintenance of proofs by
a team of people.  Being small in size helps maintenance since when
computer code does not fit the screen one has hard times understanding
it~\cite{Weinberg:1985:PCP:536771} and hence repairing it.  Moreover,
structure confines errors arising after a change to smaller text
blocks, and declares what the original author of the block being
repaired was doing effectively implementing a form of check pointing
and documentation.  All that works best if the tactics used to fill in
the blocks have a predictable behavior: fail early and locally when a
breaking change takes place.

This chapter covers the tools to structure and tie proofs up and discusses
some of the good practices that made the development of the \mcbMC{}
library possible.

% \section{maybe this should go back to the previous chapter, since it is CH}
% 
%   The proof commands we have mentioned in chapter~\ref{ch:proofs} can all be
% explained in terms of the proof terms they produce behind the scenes.
% For example, \C{case: n} provides a much more compact syntax
% for \C{(match .. with .. end)} and it produces a
% \C{match} expression with the right
% shape by looking at the type of \C{n}.  If \C{n} is a natural number, then there
% are two branches; the one for the \C{S} constructor carries an argument of type
% \C{nat}, the other one is for \C{0} and binds no additional term.
% The \C{case:} tactic is general enough to work with any inductive data type
% and inductive predicate. Note that this tactic is known as
% \emph{destruction}, since it transforms an object of an inductive type
% back into the arguments that this object was constructed from (using
% the constructors of the type).
% % [DG] I've introduced the word "destruction", since you use it later
% % on.
% 
% The \C{apply:} tactic generates an application.  For example, \C{apply: addnC}
% generates the term \C{(addnC t1 t2)} by figuring out the correct values of
% \C{t1} and \C{t2}, or opening new goals when this cannot be done, i.e.,
% if the lemma takes in input proofs, like \C{contraL}.
% 
% There is a list of proof commands that are shorthands for \C{apply:}
% and are only worth mentioning here briefly. \C{split} proves a conjunction
% by applying the \C{conj} constructor; \C{left} and \C{right} prove a
% disjunction by applying \C{or\_introl} and \C{or\_intror} respectively;
% \C{exists t} proves an existentially quantified formula by providing
% the witness \C{t} and, later, a proof that \C{t} validates the predicate.
% Finally \C{reflexivity} proves an equality by applying \C{erefl}.
% 
% The only primitive constructor that remains without an associated proof command
% is \C{(fun .. => ..)}.  Operationally, what the $\to_I$ and
% $\forall_I$ logical rule do is to introduce into the proof context a
% new entry.  So far we either expressed this step at the beginning of proofs
% by placing such items just after the name of the lemma being proved, or
% just after a \C{case:} or \C{elim:} proof command.  The current section
% expands on this subject covering the full management of the proof context.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Managing the proof context}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bookkeeping: goals as stacks}\label{ssec:stack}
\index[concept]{goal stack model}

The presentation we gave so far of proof commands like \C{case: n => [|m]}
is oversimplified.  While \C{case} is indeed the proof command in
charge of performing case analysis, the ``\C{: n}'' and ``\C{=> [|m]}''
parts are decorators to prepare the goal and post-process the result of
the proof command.  These decorators perform what we typically call
\emph{bookkeeping}: actions that are necessary in order to obtain readable and
robust proof scripts but that are too frequent to benefit from a more verbose
syntax.  Bookkeeping actions do convey a lot of information, like where
names are given to assumptions, but also let one deal with annoying details
using a compact, symbolic, language.  Note that all bookkeeping actions
correspond to regular, named, proof commands.  It is the use one makes of them
that may be twofold: a case analysis in the middle of a proof may start two
distinct lines of reasoning, and hence it is worth being noted explicitly with
the \C{case} word. Conversely, breaking a pair into two pieces is
usually not a significant, meaningful step in a proof: hence the
possibility to use a lightweight and compact syntax for this bookkeeping
action, instead of an explicit mention of the \C{case} tactic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pulling from the stack}
\index[ssr]{\C{tactic => ..}}

Let's start with the post-processing phase, called \emph{introduction pattern}.
The postfix ``\C{=> ...}'' syntax can be used in conjunction with any proof
command, and it performs a sequence of actions on the first
assumption or variable that appears in the goal (i.e., on
\C{A} if the goal has the form \C{A -> ...}, or on
\C{x} if the goal has the form \C{forall x, ...}).
With these looking glasses, the goal becomes a
\emph{stack}. Take for example this goal:

\begin{coqout}{name=Stack}{}
========================
forall xy, prime xy.1 -> odd xy.2 -> 2 < xy.2 + xy.1
\end{coqout}

Before accessing the assumption \C{(prime xy.1)}, one has to name the
bound variable \C{xy}, exactly as one can only access a stack from its top.
The execution of \C{=> xy pr\_x odd\_y} is just the composition of
\C{=> xy} with \C{=> pr\_x} and finally \C{=> odd\_y}.  Each action
pulls an item out of the stack and names it.  The \C{move} proof
command does nothing, so we use it as a placeholder
for the postfix \C{=>} bookkeeping action:

\begin{coq-left}{}{width=7cm}
move=> xy pr_x odd_y.
$~$
$~$
$~$
$~$
\end{coq-left}
\begin{coqout-right}{name=Stack1}{width=5cm}
 xy : nat * nat
 pr_x : prime xy.1
 odd_y : odd xy.2
========================
 2 < xy.2 + xy.1
\end{coqout-right}

Now, en passant, we would like to decompose \C{xy} into its first
and second component.  Instead of the verbose \C{=> xy; case: xy => x y},
we can use the symbolic notation \C{[]} to perform such action.

\begin{coq-left}{}{width=7cm}
move=> [x y] pr_x odd_y.
$~$
$~$
$~$
$~$
\end{coq-left}
\begin{coqout-right}{name=Stack2}{width=5cm}
 x, y : nat
 pr_x : prime (x,y).1
 odd_y : odd (x,y).2
========================
 2 < (x,y).2 + (x,y).1
\end{coqout-right}
\index[ssr]{\C{tactic => ..}!\C{=> [ .. $\mid$ .. ]} (case)}

We can place the \C{/=} switch to tell \Coq{} to reduce the formulas on
the stack, before introducing them in the context, and obtain:

\begin{coq-left}{}{width=7cm}
move=> [x y] /= pr_x odd_y.
$~$
$~$
$~$
$~$
\end{coq-left}
\begin{coqout-right}{name=Stack2}{width=5cm}
 x, y : nat
 pr_x : prime x
 odd_y : odd y
========================
 2 < y + x
\end{coqout-right}

We can also process an assumption through a lemma; when a lemma is
used in this way, it is called a \emph{view}. 
%This feature is of
%general interest, but it is especially useful in the context of boolean
%reflection, as described in section~\ref{sec:views}. For example,
The
\C{prime\_gt1} lemma states that \C{(prime p -> 1 < p)} for any \C{p}, and we can
use it as a function to obtain a proof of \C{(1 < x)}  from a proof
of \C{(prime x)}.
\index[concept]{view}


\begin{coq-left}{}{width=7cm}
move=> [x y] /= /prime_gt1-x_gt1 odd_y.
$~$
$~$
$~$
$~$
\end{coq-left}
\begin{coqout-right}{name=Stack2}{width=5cm}
 x, y : nat
 x_gt1 : 1 < x
 odd_y : odd y
========================
 2 < y + x
\end{coqout-right}
\index[ssr]{\C{tactic => ..}!\C{=> /=} (simplification)}

The leading \C{/} makes \C{prime\_gt1} work as a function instead of
as a name to be assigned to the top of the stack.  The \C{-} has no effect but
to visually link the function with the name \C{x_gt1} assigned to its output.
Indeed \C{-} can be omitted.

One could also examine \C{y}: it can't be \C{0}, since it would contradict
the assumption saying that \C{y} is \C{odd}.

\begin{coq-left}{}{width=7cm}
move=> [x [//|z]] /= /prime_gt1-x_gt1.
$~$
$~$
$~$
$~$
\end{coq-left}
\begin{coqout-right}{name=Stack2}{width=5cm}
 x, z : nat
 x_gt1 : 1 < x
 ========================
 ~~ odd z -> 2 < z.+1 + x
\end{coqout-right}
\index[ssr]{\C{tactic => ..}!\C{=> /view} (view application)}
\index[ssr]{\C{tactic => ..}!\C{=> //} (close trivial goals)}

This time, the destruction of \C{y} generates two cases for the two
branches; hence the \C{[ .. | .. ]} syntax.  In the first one, when
\C{y} is \C{0},
the \C{//} action solves the goal, by the trivial means
of the \C{by []} terminator.  In the second branch we name \C{z} the
new variable (the predecessor of what used to be called \C{y}).
Since \C{y} is destructed we could have reused that name for
its predecessor, as it is often done in the \mcbMC{} library.
In fact, the boolean predicate \C{odd} is defined by case analysis,
and induction, on its argument of type \C{nat}:
\begin{coq}{}{}
Fixpoint odd n := if n is n'.+1 then ~~ odd n' else false.
\end{coq}
Therefore, when applied to \C{z.+1}, it simplifies to \C{\~\~ odd z}.

Now, the fact that \C{z} is even is not needed to conclude, so we can
discard it by giving it the \C{\_} dummy name.~\footnote{
If an assumption has already a name, it can be discarded by
writing its name in curly braces: e.g. \C{move=> \{x_gt1\}}.
\index[ssr]{\C{tactic => ..}!\C{=> \{name\}} (disposal)}
}

\begin{coq}{}{}
by move=> [x [//|z]] /= /prime_gt1-x_gt1 _; apply: ltn_addl x_gt1.
\end{coq}

We finally conclude with the \C{apply:} command. In the example just
shown, we have used it with two arguments: a function and its last
argument. In fact, the lemma \C{ltn\_addl} looks as follows:

\begin{coq-left}{}{width=4cm}
About ltn_addl.
$~$
$~$
\end{coq-left}
\begin{coqout-right}{}{width=8cm}
ltn_addl : forall m n p : nat,
  m < n -> m < p + n
Arguments m, n are implicit
\end{coqout-right}

\C{apply:} automatically fills in the blanks between the function
\C{ltn\_addl}
(the lemma name) and the given argument \C{x_gt1}.
Since we are passing \C{x\_gt1}, the
variable \C{m} takes the value \C{1}.  The conclusion of \C{ltn\_addl}
hence unifies with \C{(2 < z.+1 + x)} because both \C{+} and \C{<} are
defined as programs that compute: Namely, addition exposes a \C{.+1} by
reducing to \C{2 < (z + x).+1}; then \C{<} (or, better, the underlying
\C{<=}) eats a successor from both sides, leading to \C{1 < z + x},
which looks like the conclusion of the lemma we apply.

Here we have shown all possible actions one can perform in an intro
pattern, squeezing the entire proof into a single line.  This has
to be seen both as an opportunity and as a danger: one can easily
make a proof unreadable by performing too many actions in the bookkeeping
operator \C{=>}.  At the same time, a trivial sub-proof like this one
should take no more than a line, and in that case one typically
sacrifices readability in favor of compactness: what would you learn by
reading a trivial proof?  Of course,
finding the right balance only comes with experience. As a rule
of thumb: follow the granularity of how you wound naturally
read your script to someone else.

\gotcha{The case intro pattern \C{[..|..]} obeys an exception: when it is the first
item of an intro pattern, it does not perform a case analysis, but only branch
on the subgoals.
%\footnote{not the status quo, but I prefer this exception to the current one}
Indeed in \C{case: n => [|m]} only one case analysis is performed.}

% [DG] The last two paragraphs (including the gotcha) seem direly in
% need of expanding. I can at most guess what the gotcha is talking
% about; it appears to be the double meaning of a
% [ ... | ... | ... ] that is placed after the => tactical depending
% on whether it is in the first position ("branching") or not
% ("destructing"), as in ยง5.4 in the \mcbSSR{} manual. I cannot say
% the manual completely clears things up for me, but at least it
% sounds like what the gotcha is warning about. I think we need at
% least one example of each of the possible meanings, along with an
% example of how to enforce the destructing meaning in the first
% position (using the "-" switch?).
%
% On a lesser issue, the word "intro pattern" is used but not
% explained; this holds even for the \mcbSSR{} manual! Is an intro
% pattern just anything that laeds to a new constant in the context?
% assia : TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Working on the stack}

The stack can also be used as a workplace.  Indeed, there is no need
to pull all items from the stack.  If we take the previous example:

\begin{coqout}{name=Stack}{}
========================
forall xy, prime xy.1 -> odd xy.2 -> 2 < xy.2 + xy.1
\end{coqout}
and we stop just after applying the view, we end up in a valid state:

\begin{coq-left}{}{width=6cm}
move=> [x y] /= /prime_gt1.
$~$
$~$
\end{coq-left}
\begin{coqout-right}{name=Stack2}{width=6cm}
 x, y : nat
 ========================
 1 < x -> odd y -> 2 < y + x
\end{coqout-right}

One can also chain multiple views on the same stack item:

\begin{coq-left}{}{width=6cm}
move=> [x y] /= /prime_gt1/ltnW.
$~$
$~$
\end{coq-left}
\begin{coqout-right}{name=Stack2}{width=6cm}
 x, y : nat
 ========================
 0 < x -> odd y -> 2 < y + x
\end{coqout-right}
\index[ssr]{\C{tactic => ..}!\C{=> /view/view} (many views)}

Two other operations are available on the top stack item: specialization
and substitution.  Let's take the following conjecture.

\begin{coqout}{}{}
========================
(forall n, n * 2 = n + n) -> 6 = 3 + 3
\end{coqout}

The top stack item is a quantified assumption.  To specialize it to, say,
\C{3} one can write as follows:

\begin{coq-left}{}{width=4cm}
move=> /(_ 3).
$~$
\end{coq-left}
\begin{coqout-right}{}{width=8cm}
========================
3 * 2 = 3 + 3 -> 6 = 3 + 3
\end{coqout-right}
\index[ssr]{\C{tactic => ..}!\C{=> /(_ arg)} (specialization)}

The idea behind the syntax here is that when we apply a view \C{v} to
the top stack item (say, \C{top}), by writing \C{/v}, we are
forming the term \C{(v top)}, whereas
when we specialize the top stack item \C{top} to an object \C{x},
by writing \C{/(_ x)}, we are forming the term \C{(top x)}.
The application of a view written \C{/v} is short for \C{/(v _)}.

When the top stack item is an equation, one can substitute it into
the rest of the goal, using \C{<-} and \C{->} for
right-to-left and left-to-right respectively.

\begin{coq-left}{}{width=4cm}
move=> /(_ 3) <-.
$~$
\end{coq-left}
\begin{coqout-right}{}{width=8cm}
========================
6 = 3 * 2
\end{coqout-right}
\index[ssr]{\C{tactic => ..}!\C{=> ->} (rewriting L2R)}
\index[ssr]{\C{tactic => ..}!\C{=> <-} (rewriting R2L)}

In other words, the arrows are just a compact syntax for rewriting,
as in the \C{rewrite} tactic, with the top assumption.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pushing to the stack}
\index[ssr]{\C{tactic : ..}}

We have seen how to pull items from the stack to the context.
Now let's see the so called \emph{discharging} operator \C{:}, performing
the converse operation.
Such operator decorates
proof commands as \C{move}, \C{case} and \C{elim}
 with actions to be performed before the command is actually run.

\gotcha{The colon symbol in \C{apply:} is not the discharging operator.
It is just a marker to distinguish the \C{apply:} tactic of
\mcbSSR{} from the \C{apply} tactic of \Coq{}.
Indeed the two tactics, while
playing similar roles, behave differently~\cite[\S 5.2 and \S 5.3]{ssrman}.}

Imagine we want to perform case analysis on \C{y} at this stage:

\begin{coqout}{name=Stack2}{}
 x, y : nat
 x_gt1 : 1 < x
 odd_y : odd y
 ========================
 2 < y + x
\end{coqout}

The command \C{case: y} is equivalent to
\C{move: y; case.} where \C{move} once again is a placeholder,
\C{: y} pushes the \C{y} variable onto the stack, and \C{case}
operates on the top item of the stack.
Pushing items on the stack is called \emph{discharging}.
% [DG] Reinstated, since you use the term.
% assia: TODO

Just before running \C{case}, the goal would look like this:

\begin{coqout}{name=Stack2}{}
 x : nat
 x_gt1 : 1 < x
 odd_y : odd y
 ========================
 forall y, 2 < y + x
\end{coqout}

However, this is not actually a well-defined state.
Indeed, the binding for \C{y} is needed by the \C{odd\_y}
context item, so \C{move: y} fails.  One has to push items onto the
stack in a valid order: first, all properties of a variable, then the
variable itself.  The correct invocation,
\C{move: y odd\_y}, pushes first \C{odd\_y} and only then \C{y} onto
the stack, leading to the valid goal

\begin{coqout}{name=Stack2}{label=lst:stack2}
 x : nat
 x_gt1 : 1 < x
 ========================
 forall y, odd y -> 2 < y + x
\end{coqout}
\index[ssr]{\C{tactic : ..}!\C{: term} (generalization)}

Via the execution of \C{case} one obtains:

\begin{coqout}{name=Stack2}{}
2 subgoals

  x : nat
  x_gt1 : 1 < x
  ========================
   odd 0 -> 2 < 0 + x

subgoal 2 is:
 forall n : nat, odd n.+1 -> 2 < n.+1 + x
\end{coqout}

Note that listing context entry names inside curly braces purges them
from the context. For instance the tactic \C{case: y \{odd\_y\}}
clears the \C{odd\_y} fact. But this would lead to a dead end in the
present proof, so we don't use it here.

\index[ssr]{\C{tactic : ..}!\C{: \{name\}} (disposal)}

One can combine \C{:} and \C{=>} around a proof command, to first prepare the
goal for its execution and finally apply the necessary bookkeeping to the
result.  For example:

\begin{coq-left}{}{width=6cm}
case: y odd_y => [|y']
$~$
$~$
$~$
$~$
$~$
$~$
\end{coq-left}
\begin{coqout-right}{name=Stack2}{width=6cm}
  x : nat
  x_gt1 : 1 < x
  ========================
   odd 0 -> 2 < 0 + x

subgoal 2 is:
 odd y'.+1 -> 2 < y'.+1 + x
\end{coqout-right}

At the left of the ``\C{:}'' operator one can also put a name for an
equation that links the term at the top of the stack before and
after the execution of the tactic. For example,
 \C{case E: y odd\_y => [|y']} leads to
the following two subgoals:

\begin{coqout}{name=Stack2}{width=6cm}
 x, y : nat
 x_gt1 : 1 < x
 E : y = 0
============================
 odd 0 -> 2 < 0 + x
\end{coqout}
\begin{coqout}{name=Stack2}{width=6cm}
 x, y : nat
 x_gt1 : 1 < x
 y' : nat
 E : y = y'.+1
========================
 odd y'.+1 -> 2 < y'.+1 + x
\end{coqout}
\index[ssr]{\C{tactic : ..}!\C{name: term} (equation)}

Lastly, one can push any term onto the stack -- whether or not this
term appears in the context. For example,
``\C{move: (leqnn 7)}'' pushes on the stack
the additional assumption \C{(7 <= 7)}.
% and thus
%``\C{move: (leqnn 7) => leq77}'' brings this assumption into the context
%under the name \C{leq77}.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Example: strengthening induction}
% \index[concept]{induction!strong}
% \index[concept]{induction!curse of values}
% \label{sec:strongind}
% 
% As an exercise we show how the \C{elim} tactic combined with the bookkeeping
% operator ``\C{:}'' lets one perform, on the fly, a stronger variant of
% induction called ``course of values'' or ``strong induction''.
% % [DG] I've never seen it called "course of values". Apparently this
% % name is mostly used in computability theory? But "strong induction"
% % appears very often.
% %assia : TODO
% 
% Claim: every amount of postage that is at least 12 cents
% can be made from 4-cent and 5-cent stamps.  The proof in the inductive
% step goes as follows.  There are obvious solutions for a postage between
% 12 and 15 cents, so we can assume it is at least 16 cents.  Since
% the postage amount is at least 16, by using a 4-cents stamp we are back
% at a postage amount that, by induction, can be obtained as claimed.\hfill$\square$
% 
% The tricky step is that we want to apply the induction hypothesis not
% on $n-1$, as usual, but on $n-4$, since we know how to turn a
% solution for a stamping amount problem $n$ to one for a problem of
% size $n+4$ (by using a 4-cent stamp).
% The induction hypothesis provided by \C{nat_ind}
% is not strong enough.  However we can use the \C{:} operator
% to \emph{load the goal} before performing the induction.
% %\footnote{See further below for explanations.}
% \index[ssr]{\C{set name := term}}
% 
% \begin{coq}{}{}
% Lemma stamps n : 12 <= n -> exists s4 s5, s4 * 4 + s5 * 5 = n.
% Proof.
% have [m leq_mn] := ubnPgeq n; elim: n => // n IHn in m leq_mn *; first by case: n => [|n] in IHn *.
% do 12! [ case: m => //= m in leq_mn * ] => _.
% case: m => [|m] in leq_mn *; first by exists 3, 0.
% case: m => [|m] in leq_mn *; first by exists 2, 1.
% case: m => [|m] in leq_mn *; first by exists 1, 2.
% case: m => [|m] in leq_mn *; first by exists 0, 3.
% case: (IHn ((16+m) - 4) _ isT) => [|s4 [s5 def_m4]].
%   by rewrite leq_subLR (leq_trans leq_mn) // addSnnS leq_addl.
% by exists s4.+1, s5; rewrite mulSn -addnA def_m4 subnKC.
% Qed.
% \end{coq}
% 
% 
% \begin{coq}{}{}
% Lemma stamps n : 12 <= n -> exists s4 s5, s4 * 4 + s5 * 5 = n.
% Proof.
% elim: n {-2}n (leqnn n) =>[|n IHn]; first by case.
% \end{coq}
% 
% Let's dissect this line 3. First of all,
% ``\C{elim: n \{-2\}n (leqnn n).}'' is shorthand for
% ``\C{move: (leqnn n). move: \{-2\}n. move: n. elim.}'' (the terms after the
% colon \C{:} are pushed to the stack, from right to left; and the
% \C{elim} tactic is applied afterwards to the top of the stack, which
% of course is the last term pushed to the stack). Let us see how these
% four steps transform the goal. At the beginning, the context and the
% goal are
% 
% \begin{coqout}{}{width=11cm}
%   n : nat
%   ============================
%   11 < n -> exists s4 s5 : nat, s4 * 4 + s5 * 5 = n
% \end{coqout}
% 
% The first of our
% four steps (``\C{move: (leqnn n)}'') pushes the additional
% assumption \C{n <= n} onto the stack (since \C{(leqnn n)} provides
% its proof); we are thus left with
% 
% \begin{coqout}{}{width=11cm}
%   n : nat
%   ============================
%   n <= n -> 11 < n -> exists s4 s5 : nat, s4 * 4 + s5 * 5 = n
% \end{coqout}
% 
% The second step (``\C{move: \{-2\}n}'') is more interesting. Recall
% that \C{move:} usually ``generalizes'' a variable (i.e., takes a
% variable appearing in the context, and binds it by the universal
% quantifier, so that the goal becomes a for-all statement). The prefix
% \C{\{-2\}} means ``all except for the 2nd occurrence in the goal''; so
% the idea is
% to generalize ``all except for the 2nd occurrence of $n$''. Of course,
% this implies that $n$ still has to remain in the context (unlike for
% ``\C{move: n}''), so the
% bound variable of the universal quantifier has to be a fresh
% variable, picked automatically by \Coq{}. For example, \Coq{} might
% pick \C{n0} for its name, and so the state after the second step will
% be:
% 
% \begin{coqout}{}{width=11cm}
%   n : nat
%   ============================
%   forall n0 : nat,
%   n0 <= n -> 11 < n0 -> exists s4 s5 : nat, s4 * 4 + s5 * 5 = n0
% \end{coqout}
% 
% Notice how the \C{n} in ``\C{n0 <= n}'' has remained un-generalized,
% since it was the 2nd occurrence of $n$ before the second step.
% 
% The third step (``\C{move: n}'') merely moves the \C{n} from the
% context to the goal. Thus, after the three ``\C{move}'' steps, we are
% left with proving the following claim:
% 
% \begin{coqout}{}{}
% forall n n0 : nat,
%   n0 <= n -> 11 < n0 -> exists s4 s5 : nat, s4 * 4 + s5 * 5 = n0
% \end{coqout}
% 
% The \C{elim} now applies induction on the top of the stack, which
% is \C{n}. The corresponding induction hypothesis \C{IHn} is:
% 
% \begin{coqout}{}{}
% IHn : forall n0 : nat,
%       n0 <= n ->
%       11 < n0 -> exists s4 s5 : nat, s4 * 4 + s5 * 5 = n0
% \end{coqout}
% 
% Of course, the \C{n} here is not the original \C{n}, but the new
% \C{n} introduced in the \C{=>[|n IHn]} pattern.
% 
% The proof continues like that:
% 
% \lstset{firstnumber=4}
% \begin{coq}{}{}
% do 12! [ case; first by [] ]. (* < 12c *)
% case; first by exists 3, 0.   (* 12c = 3 * 4c *)
% case; first by exists 2, 1.   (* 13c = 2 * 4c + 1 * 5c *)
% case; first by exists 1, 2.   (* 14c = 1 * 4c + 2 * 5c *)
% case; first by exists 0, 3.   (* 15c = 3 * 5c *)
% move=> m'; set m := _.+1; move=> mn m11.
% \end{coq}
% \lstset{firstnumber=1}
% 
% Line 4 repeats a tactic 12 times using the \C{do 12!} tactical.
% This deals with the 12 cases where \C{n0} is not greater than 11.
% \index[ssr]{\C{do n"!} (iteration)}
% 
% Lines from 5 to 8 provide solutions for 12, 13, 14 and 15 cents.
% At line 9, just after the \C{move=> m'} step, the goal is the following one:
% 
% \begin{coqout}{}{}
% 1 subgoal
%   
%   n : nat
%   IHn : forall n0 : nat, n0 <= n -> 
% 	  11 < n0 -> exists s4 s5 : nat, s4 * 4 + s5 * 5 = n0
%   m' : nat
%   ============================
%   m'.+3.+4.+4.+4 < n.+1 ->
%     11 < m'.+4.+4.+4.+4 ->
%       exists s4 s5 : nat, s4 * 4 + s5 * 5 = m'.+4.+4.+4.+4
% \end{coqout}
% 
% This goal is hard to read because of the many \C{.+1} piling
% up. \mcbMC{} has the notations \C{.+1},
% \C{.+2}, \C{.+3} and \C{.+4} pre-defined, but not \C{.+5} and higher,
% hence what could be in principle displayed as \C{.+15} is actually
% displayed as \C{.+3.+4.+4.+4}.
% 
% We resort to the \C{set m := _.+1} tactic to build an abbreviation
% named \C{m} for the terms that match the pattern \C{_.+1}. As for the
% \C{rewrite} tactic, the goals is traversed from left to right, outside
% in, in order to find a term that matches the pattern.
% 
% The first term encountered is \C{m'.+3.+4.+4.+4.+1}. Recall that
% the \C{<} infix notation hides an \C{.+1} on the left. Once
% the term above is extracted from its context the notation
% for \C{<} does not apply anymore: the abbreviation 
% displays as \C{(m := m'.+4.+4.+4.+4 : nat)} in the context,
% and the top assumption as \C{m <= n.+1}.
% 
% After naming \C{mn} and \C{m11} the goal is the following one"
% 
% \begin{coqout}{}{}
%   n : nat
%   IHn : forall n0 : nat, n0 <= n ->
%           11 < n0 -> exists s4 s5 : nat, s4 * 4 + s5 * 5 = n0
%   m' : nat
%   m := m'.+4.+4.+4.+4 : nat
%   mn : m <= n.+1
%   m11 : 11 < m
%   ============================
%   exists s4 s5 : nat, s4 * 4 + s5 * 5 = m
% \end{coqout}
% 
% % 
% % Line 9 uses the \C{set} proof command to define a new
% % constant in the context. For example, ``\C{set a := 24 * 11.}'' would
% % create a new ``\C{a := 24 * 11 : nat}'' item in the context. The
% % command also tries to substitute the newly-defined constant for its
% % appearances in the goal; for example, ``\C{set a := 11.}'' would not
% % only create a new ``\C{a := 11 : nat}'' in the context, but also
% % replace the ``\C{11 < m'.+4.+4.+4.+4}'' in the goal\footnote{Don't be
% % surprised by the fact that an addition of 16 is circumscribed by four
% % additions of 4. By default, \mcbMC{} has the notations \C{.+1},
% % \C{.+2}, \C{.+3} and \C{.+4} pre-defined, but not \C{.+5} and higher.}
% % by an ``\C{a < m'.+4.+4.+4.+4}''. In our example above
% % (``\C{set m := _.+1}''), we are using the
% % \C{set} command with a wildcard; this captures the first term of the
% % form \C{_.+1} appearing in the goal and denotes it by \C{m}, replacing
% % it by \C{m} on the goal. In our case, this first term is
% % \C{m'.+4.+4.+4.+4} (which is just syntactic sugar for
% % \C{m'.+1.+1.....} with 16 appearances of \C{.+1})\footnote{This is
% % slightly counterintuitive, as you
% % might instead believe it to be \C{m'.+3.+4.+4.+4}. But keep in mind
% % that \C{m'.+3.+4.+4.+4 < n.+1} is just syntactic sugar for
% % \C{m'.+4.+4.+4.+4 <= n.+1}.},
% % % [DG] I hope I got these subtleties right. That said, I think it
% % % would be better to first introduce the "set" tactic in a less
% % % confusing example.
% % %assia : TODO
% % and so the name \C{m} is given to this term. We could have achieved
% % the same goal using ``\C{set m := m'.+4.+4.+4.+4}''.
% % \par
% % Further details about the \C{set} tactic can be found in
% % \cite[\S 4.2]{ssrman}; let us only mention the (by now) habitual
% % variant \C{set a := \{k\}(pattern)}, which defines a new constant
% % \C{a} to equal the first subterm of the goal that matches the pattern
% % \C{pattern}, but then replaces \emph{only the \(k\)-th} appearance
% % of this subterm in the goal by \C{a}. As usual, if the
% % pattern-matching algorithm keeps finding the wrong subterms, it is
% % always possible to completely specify the subterm, leaving no
% % wildcards.
% 
% The proof continues as follows:
% 
% \lstset{firstnumber=10}
% \begin{coq}{}{}
% case: (IHn (m-4) _ isT) => [|s4 [s5 def_m4]].
%   by rewrite leq_subLR (leq_trans mn) // addSnnS leq_addl.
% by exists s4.+1, s5; rewrite mulSn -addnA def_m4 subnKC.
% Qed.
% \end{coq}
% \lstset{firstnumber=1}
% 
% Line 10 instantiates the induction hypothesis with the value
% \C{(m-4)}, with a placeholder for a missing proof of \C{(m-4 < n)},
% and with a proof that \C{(11 < m-4)}. The proof given for
% \C{(11 < m-4)} can be just \C{isT} since the type \C{(11 < m-4)} 
% \emph{computes} to \C{true} since the value of \C{m} is larger than 11.
% The missing proof
% of \C{(m-4 < n)}  becomes the first
% subgoal, solved at line 11.
% 
% The induction hypothesis yields
% \C{exists s4 s5 : nat, s4 * 4 + s5 * 5 = m-4}. The introduction
% pattern in line 10 gives names to the values of \C{s4} and \C{s5}
% whose existence is thus guaranteed, and named \C{def_m4} the
% assumption
% \C{s4 * 4 + s5 * 5 = m-4}. In other words, it gives names \C{s4} and
% \C{s5} to the quantities of 4-cent and 5-cent stamps needed to cover
% the amount of postage \C{(m-4)}, as well as to the fact that they do
% cover this exact amount of postage. The intro pattern to name
% \C{s5} needs an extra destructing pattern (square bracket) because
% \C{(exists s4 s5, ...)} is just a notation for
% \C{(exists s4, exists s5, ...)} hence two existential
% quantifiers need to be actually dealt with.
% 
% Line 12 gives the final solution: by using one extra 4 cents stamp
% we can appeal to the induction hypothesis.
% 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Structuring proofs, by examples}

So far we've only tackled simple lemmas; most of them did admit a one line
proof.  When proofs get longer \emph{structure} is the best ally in making
them readable and maintainable.  Structuring proofs means identifying
intermediate results, factoring similar lines of reasoning (e.g., symmetries),
signaling crucial steps to the reader, and so on.  In short, a
proof written in \Coq{} should share its structure and main steps
with the same proof written on paper.

The first subsection introduces the \C{have} tactic, that is the key
to structure proofs into intermediate steps.  The second subsection
deals with the ``problem'' of symmetries.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Primes, a never ending story}\label{sec:infprimes}
\index[concept]{forward reasoning}

Saying that primes are infinite can be phrased as: for any natural number
$m$, there exists a prime greater than $m$.  The proof of this claim goes like
that: every natural number greater than 1 has at least one prime divisor.  If
we take $m! + 1$, then such prime divisor $p$ can be shown to be greater than $m$ as
follows.  By contraposition we assume $p \leq m$ and we show that $p$
does not divide $m!+1$.
Being smaller than $m$, $p$ divides $m!$, hence to divide $m!+1$, $p$ should divide
$1$; that is not possible since $p$ is prime, hence greater than 1.
\hfill$\square$



% We first show that any positive number smaller than $n$ divides $n!$.
% 
% \begin{coq}{}{}
% Lemma dvdn_fact m n : 0 < m <= n -> m %| n`!.
% Proof.
% case: m => //= m; elim: n => //= n IHn; rewrite ltnS leq_eqVlt.
% by case/orP=> [/eqP-> | /IHn]; [apply: dvdn_mulr | apply: dvdn_mull].
% Qed.
% \end{coq}
% 
% After the first line the proof state is the following one:
% 
% \begin{coqout}{}{}
% m, n : nat
% IHn : m < n -> m.+1 %| n`!
% ========================
% (m == n) || (m < n) -> m.+1 %| (n.+1)`!
% \end{coqout}
% The case analysis rules out \C{(m = 0)}, and simplifies the hypothesis
% to \C{(m <= n)}.  Recall that \C{(x <= y <= z)} is a notation for \C{((x <= y) &&
% (y <= z))}; hence when the first inequality evaluates to true (e.g. when \C{x}
% is 0) the conjunction simplifies to the second conjunct.  The \C{leq_eqVlt}
% rewrite rule rephrases \C{<=} as a disjunction (the capital \C{V} letter
% is reminiscent of $\lor$).
% 
% When we reason by cases on the top assumption, line 4, we face two goals, both
% easy to solve if we look at the development of the factorial of \C{n.+1},
% i.e., \D{(n.+1 * n`!)}.  The former amounts to showing that
% \D{(n.+1 \%| n.+1 * n`!)}, while the latter to showing that \D{(m.+1 \%| n.+1 *
% n`!)} under the (inductive) hypothesis that \D{(m.+1 \%| n`!)}.
% 
% What is paradigmatic in this little proof is the use of the \emph{goal stack
% as a work space}.  In other words the proof script would be much more involved
% if we started by introducing in the proof context all assumptions.
% \\
% 
% We can now move to the proof of the main result.  

We state our theorem using a
``synonym'' of the exists quantifier that is specialized to carry two
properties.  This way the statement is simpler to destruct: with just one
case analysis we obtain the witness and the two properties.

\begin{coq}{}{title=Ex2}
Inductive ex2 A P Q : Prop := ex_intro2 x of P x & Q x.
Notation "exists2 x , p & q" := (ex2 (fun x => p) (fun x => q)).
\end{coq}
\index[coq]{\C{exists2}}

We also resort to the following notations and lemmas.

\begin{coq}{}{title=Tools}
Notation "n `!" := (factorial n).
Lemma fact_gt0 n : 0 < n`!.
Lemma dvdn_fact m n : 0 < m <= n -> m %| n`!.
Lemma pdivP n : 1 < n -> exists2 p, prime p & p %| n,
Lemma dvdn_addr m d n : d %| m -> (d %| m + n) = (d %| n).
Lemma gtnNdvd n d : 0 < n -> n < d -> (d %| n) = false.
\end{coq}
\index[coq]{\C{`!}}

\noindent
The first step is to prove that $m! + 1$ is greater than $1$, a triviality.
Still it gives us the occasion to explain the \C{have} tactic, which lets us
augment the proof context with a new fact, typically an intermediate step of
our proof.
\index[ssr]{\C{have name : type}}
\index[ssr]{\C{have name : type by tactic}}

\begin{coq}{}{}
Lemma prime_above m : exists2 p, m < p & prime p.
Proof.
have m1_gt1: 1 < m`! + 1.
  by rewrite addn1 ltnS fact_gt0.
\end{coq}

Its syntax is similar to the one of the \C{Lemma} command: it takes a name, a
statement and starts a (sub) proof. 
%Since the proof is so short, we will
%put it on the same line, and remove the full stop.

The next step is to use the \C{pdivP} lemma to gather a prime divisor of
\D{m`!.+1}.  We end up with the following, rather unsatisfactory, script.

\begin{coq}{}{}
Lemma prime_above m : exists2 p, m < p & prime p.
Proof.
have m1_gt1: 1 < m`! + 1.
  by rewrite addn1 ltnS fact_gt0.
case: (pdivP m1_gt1) => [p pr_p p_dv_m1].
\end{coq}

It is unsatisfactory because in our paper proof what plays an
interesting role is the \C{p} that we obtain in the second line,
and not the \C{m1_gt1} fact we proved as an intermediate fact.

We can resort to the flexibility of \C{have} to obtain a more
pertinent script: the first argument to \C{have}, here a name, can
actually be any introduction pattern, i.e. what follows
the \C{=>} operator, for example a view application.
In the light of that, the script can be
rearranged as follows.

\begin{coq}{}{}
Lemma prime_above m : exists2 p, m < p & prime p.
Proof.
have /pdivP[p pr_p p_dv_m1]: 1 < m`! + 1.
  by rewrite addn1 ltnS fact_gt0.
exists p => //; rewrite ltnNge; apply: contraL p_dv_m1 => p_le_m.
by rewrite dvdn_addr ?dvdn_fact ?prime_gt0 // gtnNdvd ?prime_gt1.
Qed.
\end{coq}

Here the first line obtains a prime \C{p} as desired, the third
one begins to show it fits by contrapositive reasoning, and the
last one, already commented in section~\ref{sec:quantifiedst}, concludes.

As a general principle, in the proof script style we propose, a full
line should represent a meaningful reasoning step (for a human being).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Order and max, a matter of symmetry}\label{sec:leqmax}
\index[concept]{forward reasoning}
\index[concept]{symmetric argument}

It is quite widespread in paper proofs to appeal to the reader's intelligence
pointing out that a missing part of the proof can be obtained by symmetry.
The worst thing one can do when formalizing such an argument on a computer
is to use the worst invention of computer science: copy-paste (i.e.
duplication).  The language of
\Coq{} is sufficiently expressive to model symmetries, and the
\mcbSSR{} proof language provides facilities to write symmetric arguments.

We prove the following characterization of the max of two natural numbers:
\[
\forall n_1, n_2, m, \quad m \le \max(n_1,n_2)
\;\Leftrightarrow\; m \le n_1 \textrm{ or } m \le n_2
\]

The proof goes as follows: Without loss of generality we can assume that
$n_2$ is greater or equal to $n_1$, hence $n_2$ is the maximum between
$n_1$ and $n_2$.  Under this assumption it is sufficient to check
that $m \le n_2$ holds iff either $m \le n_2$ or $m \le n_1$.
The only non-trivial case is when we suppose $m \le n_1$ and
we need to prove $m \le n_2$, which holds by transitivity.\hfill$\square$

As usual we model double implication as an equality between two
boolean expressions:

\begin{coq}{}{}
Lemma leq_max m n1 n2 : (m <= maxn n1 n2) = (m <= n1) || (m <= n2).
\end{coq}

The proof uses the following lemmas.  Pay attention to the premise of
\C{orb_idr}, which is an implication.

\begin{coq}{}{title=Tools}
Lemma orP {a b : bool} : a || b -> a \/ b.
Lemma orb_idr (a b : bool) : (b -> a) -> (a || b) = a.
Lemma orbC a b : a || b = b || a.
Lemma maxn_idPl {m n} : n <= m -> maxn m n = m.
Lemma maxnC m n : maxn m n = maxn n m.
Lemma leq_total m n : (m <= n) || (n <= m).
\end{coq}

Our first attempt takes no advantage of the symmetry argument:
we reason by cases on the order relation,
we name the resulting fact on the same line
(it eases tracking where things come from) and we solve the two
goals independently.

\begin{coq}{}{}
Proof.
case: (orP (leq_total n2 n1)) => [le_n21|le_n12].
  rewrite (maxn_idPl le_n21) orb_idr // => le_mn2.
  by apply: leq_trans le_mn2 le_n21.
rewrite maxnC orbC.
rewrite (maxn_idPl le_n12) orb_idr // => le_mn1.
by apply: leq_trans le_mn1 le_n12.
Qed.
\end{coq}

After line 2, the proof status is the following one:

\begin{coqout}{}{title=Output line 2}
2 subgoals
m, n1, n2 : nat
le_n21 : n2 <= n1
========================
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)

subgoal 2 is:
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)
\end{coqout}

The first goal is simplified by
rewriting with \C{maxn_idPl}. % (as we did in section~\ref{sec:viewtac}).
Then \C{orb_idr} trivializes the main goal and generates a side condition with
an extra hypothesis we name \C{le_mn2}.

\begin{coqout}{}{title=Output before line 3,width=6.7cm}
2 subgoals
m, n1, n2 : nat
le_n21 : n2 <= n1
========================
(m <= n1) = (m <= n1) || (m <= n2)

subgoal 2 is: ...
\end{coqout}
\begin{coqout}{}{title=Output after 3,width=5.3cm}
2 subgoals
m, n1, n2 : nat
le_n21 : n2 <= n1
le_mn2 : m <= n2
========================
m <= n1

subgoal 2 is: ...
\end{coqout}

Line 4 combines by transitivity the two hypotheses to conclude.  Since it closes the
proof branch we use the prefix \C{by} to asserts the goal is solved and
visually signal the end of the paragraph.  Line 5 commutes \C{max} and \C{||}.
We can then conclude by copy-paste.


To avoid copy-pasting, shrink the proof script and finally make the
symmetry step visible we can resort to the \C{have} tactic.
In this case the statement is a variation of what we need to prove.
Remark that as for \C{Lemma}, we can place parameters, \C{x} and \C{y}
here, before the \C{:} symbol.

\begin{coq}{}{}
Lemma leq_max m n1 n2 : (m <= maxn n1 n2) = (m <= n1) || (m <= n2).
Proof.
have th_sym x y: y <= x -> (m <= maxn x y) = (m <= x) || (m <= y).
  move=> le_yx; rewrite (maxn_idPl le_yx) orb_idr // => le_my.
  by apply: leq_trans le_my le_yx.
by case: (orP (leq_total n2 n1)) => /th_sym; last rewrite maxnC orbC.
Qed.
\end{coq}

The proof for \C{th_sym} is the text we were copy-pasting in the
previous script, while here it is factored out.
Once we have such extra fact in our context we reason by cases on
the order relation and we conclude.  Remark that the last line instantiates
\C{th_sym} \emph{in each branch} using the corresponding
hypothesis on \C{n1} and \C{n2} generated by the case analysis.
If we stop just before \C{; last rewrite ...} these are the (symmetric)
goals:

\begin{coqout}{}{}
2 subgoals
m, n1, n2 : nat
th_sym : forall x y : nat,
         y <= x -> (m <= maxn x y) = (m <= x) || (m <= y)
========================
(m <= maxn n1 n2) = (m <= n1) || (m <= n2) ->
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)

subgoal 2 is:
(m <= maxn n2 n1) = (m <= n2) || (m <= n1) ->
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)
\end{coqout}
The instance of \C{th_sym} is is exactly what is needed in the
first branch, while the last goal requires the commutativity of
\C{max} and \C{||}.

We can further improve the script.  For example we could rephrase
the proof putting in front the justification of the symmetry, and
then prove one case when we pick $x$ to be smaller than $y$.

\begin{coq}{}{}
Lemma leq_max m n1 n2 : (m <= maxn n1 n2) = (m <= n1) || (m <= n2).
Proof.
suff th_sym x y: y <= x -> (m <= maxn x y) = (m <= x) || (m <= y).
  by case: (orP (leq_total n2 n1)) => /th_sym; last rewrite maxnC orbC.
move=> le_yx; rewrite (maxn_idPl le_yx) orb_idr // => le_my.
by apply: leq_trans le_my le_yx.
Qed.
\end{coq}
\index[ssr]{\C{suff} also \C{suffices}}
The \C{suff} tactic (or \C{suffices})
is like \C{have} but swaps the two goals.

Note that here the sub proof is now the shortest paragraph.
This is another recurrent characteristic of the proof script style
we adopt in the \mcbMC{} library.

There is still a good amount of repetition in the current script.
In particular the main conjecture has been almost copy-pasted in
order to invoke \C{have} or \C{suff}.  When this repetition
is undesirable, i.e. the statement to copy is large, one
can resort to the \C{wlog} tactic (or \C{without loss}).

\begin{coq}{}{}
Lemma leq_max m n1 n2 : (m <= maxn n1 n2) = (m <= n1) || (m <= n2).
Proof.
wlog le_n21: n1 n2 / n2 <= n1  => [th_sym|].
  by case: (orP (leq_total n2 n1)) => /th_sym; last rewrite maxnC orbC.
rewrite (maxn_idPl le_n21) orb_idr // => le_mn2.
by apply: leq_trans le_mn2 le_n21.
Qed.
\end{coq}
\index[ssr]{\C{wlog} also \C{without loss}}

Thanks to \C{wlog} one only needs to write the statement of the extra
assumption, here \C{n2 <= n1}, and which portion of the context needs
to be abstracted, here \C{n1} and \C{n2}.
The two goals to be proved are the following ones:

\begin{coqout}{}{}
m, n1, n2 : nat
th_sym : forall n1 n2 : nat, n2 <= n1 ->
           (m <= maxn n1 n2) = (m <= n1) || (m <= n2)
========================
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)
\end{coqout}

\begin{coqout}{}{}
m, n1, n2 : nat
le_n21 : n2 <= n1
========================
(m <= maxn n1 n2) = (m <= n1) || (m <= n2)
\end{coqout}

To keep the script similar to the previous one, we named explicitly
\C{th_sym}, but one typically leaves the assumption on the stack
and omit the trailing intro pattern \C{=> [th_sym|]}.

Shrinking proof scripts is a never ending game.  The impatient reader can
jump to the next section to see
how intro patterns can be used to squeeze the last two lines into a
single one.  In the end, this proof script consists of three steps:
the remark that we can
assume \C{(n2 <= n1)} without losing generality; its justification in
terms of totality of the order relation and commutativity of \C{max}
and \C{||}; and the final proof, by transitivity, in the case when
the \C{max} is \C{n1} due to the extra assumption.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Partially applied views}

A less important, but very widespread, feature of the \mcbSSR{}
proof language can be used to shrink the proof even further.
In the previous proof script at line 5 we had to name \C{le_mn2}
the new assumption only for the
purpose of referring to it in the immediately following transitivity step.
We can avoid this by using \C{leq_trans} as a view.


\begin{coq}{}{}
Lemma leq_max m n1 n2 : (m <= maxn n1 n2) = (m <= n1) || (m <= n2).
Proof.
wlog le_n21: n1 n2 / n2 <= n1 => [th_sym|].
  by case: (orP (leq_total n2 n1)) => /th_sym; last rewrite maxnC orbC.
by rewrite (maxn_idPl le_n21) orb_idr // => /leq_trans->.
Qed.
\end{coq}

The statement of
\C{leq_trans} is \C{(forall c a b, a <= c -> c <= b -> a <= b)} and
we use it to transform the top assumption \C{(m <= n2)} that fits in
the place of the first proof argument \C{(a <= c)}.  Note that
the \C{leq_trans} expects a second proof argument, and that its type
would fix \C{b}, that is otherwise unspecified. As a result \C{b}
stays quantified. We can put a
full stop just before the terminating~ \C{->} in order to observe the shape of
the top assumption before rewriting it:

\begin{coq}{}{}
(forall b, n2 <= b -> m <= b) -> m <= n1.
\end{coq}

Rewriting the top assumption left to right fixes \C{b} to \C{n1}, trivializes
the goal \C{(m <= n1)} to \C{true} and opens the side
condition \C{(n2 <= n1)} which is trivial.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof maintenance: a matter of style}

The \mcbMC{} library started from a few files from the Four Color
Theorem in 2006 and it was developed over 6 years by 10 people to
support the formal proof of the Odd Order Theorem, completed in 2012.
At the time of writing it has reached the size of nearly 100 files and
100 KLOC, and has been maintained for over 12 years.

During this maintenance period breakage did happen, and breakage will happen
again in the future. Things break for many reasons, some of which are even not
under the control of the author of a formal proof. First of all  \Coq{} is an
actively developed research software and changes in behavior, for the better,
are hard to avoid and do happen in practice. Second nobody gets his definitions
perfect at the first attempt in a formalization. Definitions and lemmas get
improved over time.  As a consequence having to repair and adapt proof scripts
is an activity one can hardly avoid.  For example around 2009 the way algebraic
structures are represented changed substantially with a major impact on most
of the files.

The maintenance cost of the  \mcbMC{} library and the proof of the
Odd Order Theorem were kept under control thanks to the disciplined
way in which the formal proofs are written.
In this section we cover some of the aspects that we believe do
contribute in a substantial way to write maintainable formal proofs.
Coming up with a list of unobjectionable golden rules would be quite pretentious,
and probably impossible to do, but we will anyway try to give some
advice out of our experience.

The first and most important one is that formal proofs are just computer code
about mathematical proofs, hence \emph{good practices in both domains}
are likely to apply to formal proofs too.

For example one can claim that a well written pen and paper proof makes evident
and clear what is interesting and hides via suitable abstractions uninteresting
details. Finding the right abstraction is crucial in formal proofs too, and the
related concept of triviality is key to keep the desired level of abstraction.
More specifically, each mathematical object comes with properties that are
pervasively and silently used in each and every proof. In order to make a
formal proof look like pen and paper one, this conciseness has to be retained,
or the noise of formality risks hiding the salient part of the proof.

A striking similarity lies in the usability of lemmas
compared to the one of computer code procedures (functions, or methods).
It is hard to invoke a procedure that requires
passing too many arguments to it. Even worse if the order (or the name)
of these arguments is hard to remember.
In order to ease invoking procedures related data is often
grouped in packages that are passed all together,
and the order of arguments is standardized across the whole codebase.
Similarly the statement of a lemma can be designed so that it
minimizes the information one has to explicitly provide
in order to use it in conjunction with a specific tactic.

Finally, it is common sense to organize computer code
in components with separate concerns and well defined interfaces:
Unstructured code is  well known to be hard to read, understand and
locally modify.
Also, mathematical theories are often organized and structured, for
example by algebraic means. The axioms characterizing structures
are not unlike interfaces for computer code.
Identifying the right components and their interfaces is key in a library of
formal proofs too and is the subject of Chapter~\ref{ch:hierarchy}.

\subsection{On the readability of formal proofs}

A desirable characteristic of formal proofs is readability,
there is no questioning that: How can a proof be maintained if
it cannot even be read?

It is not uncommon for novice users to try to achieve readability by
writing a single instruction per line, and relying on the capability
of user interfaces to step trough each and every step in order to
understand what is going on. We call these proofs \emph{vertical proofs}:
they are lengthy
and their structure, when present, is hard to find
because, at a glance, one can hardly look at the entire text.
It is a bit like
typesetting a book by breaking the line after each an every word: in
order to find the action the subject is performing one would need to
turn page.  
Lengthy proofs without an apparent structure are hard to
maintain: even manipulating their text is cumbersome. The last, and
most important observation drawn from vertical proofs, is that it is
not the ``atomicity'' of steps that makes the proof readable, but
rather the fact that goals, that are intermediate statements, are
more readable than proof steps. \\
Conversely, an expert \mcbSSR{} user is typically capable of squeezing
into a single line so many steps that nobody could understand the
resulting \emph{horizontal proof}, even if it requires no page flipping.
The game of ``compressing proofs''
is a typical exercise to learn the very articulate \mcbSSR{} proof
language, and it is often
mistaken by students as the recommended way of writing proofs in the
large. Horizontal proofs are indeed short, hence handy to
manipulate, but their poor readability is not always helping maintenance.\\
The funny fact is that the tension between horizontal and vertical
proofs is totally artificial: there is no need to pick one direction and only
one in a proof. Indeed, in order to obtain a readable and maintainable proof
one can mix both styles! Another way to put it is that in a proof one
can write readable-but-lengthy proof script for interesting steps
and obscure-but-compact scripts to deal with the bureaucracy of side conditions.

An undoubtedly \emph{readable proof step} is a declarative step asserting,
in the proof text, a statement that holds. For example by using the
\C{have} tactic. This step is as readable as the statements of
theorems is: It goes without saying that curating the way statements
can be written plays a main role in readability. Moreover, given that
the statement is part of the proof text, one does not need
the assistance of \Coq{} in order to see it.\\
A very related source of unreadability
is the choice of meaningless names for theorems, variables or
hypotheses, e.g. \C{H42}. By asking \Coq{} to display the goal one can see
what the 42nd hypothesis is about, but without that one can hardly
imagine. A name like \C{leq_nm} is a more suggestive name for an
assumption stating that \C{n} is smaller than \C{m}.
Similarly \C{n} is a good name for a number, \C{A} for a set or a
matrix, \C{s} for a sequence, and \C{G} for a group while \C{x7'}
is not going to help the reader much.

\emph{Obscure proof steps} are not declarative: They act on a specific proof
state and perform a precise manipulation. For example ``apply
commutativity to the first addition on the right hand side of the
goal''. Without seeing the actual goal, one cannot
understand what the precise action is actually achieving.
Another characteristic of these steps is the use of symbolic
notations for some operations, such as getting rid of trivial goals
via \C{//}, that help squeezing even more actions in a single line.
One can only follow and understand a few obscure steps without the help
of \Coq{}.\\
Still a few tricks to help reading these steps can be given. Symbols
can be grayed out, the whole point of having symbolic notation is
indeed to have a less intrusive syntax for an uninteresting
manipulation.
In addition to that in the \mcbMC{} library short names are usually
reserved for uninteresting theorems, e.g. \C{addnCA} to shuffle summands
around. What is left is the salient contents of the obscure step.

To sum up our view: Readable proofs are structured by forward steps,
and their length is kept under control by proving these steps in a
compact way.

Is this way of writing proofs tied to the technology we have today?\\
While one can imagine a future where only declarative, readable, steps
are part of a formal proof, and all their justifications can be
silently omitted thanks to automation, we are clearly not there today.
When a gap cannot be filled in automatically having the ability to do
it by hand, rather than by declaring unnatural intermediate steps just to
drive the system, proved to be quite effective since they way the
machine is guided is more direct and precise.  Also, and more
importantly in the long term, explicit proof steps provide a solid
ground for repair, while their absence may turn out to be an impeding
factor for maintenance.

An advice one can hardly imagine to become irrelevant in the future
is the following one. Structure formal proofs as they are structured on paper.
Then, when filling the blocks, use one line of formal text to perform one
meaningful proof step for a human reader, and use the best tools you have to
achieve that: At worse, if the step is too obscure, the reader will
ignore the text, and just see what happens by executing it.\\
In other words, the flow of a proof script should be homogeneous and
distill human-readable information at a suitable, regular pace.


\subsection{Fail early and locally to ease repair}

There is a trade off between formal proofs that are robust in face of
changes and formal proofs that are easy to repair. 
With robustness we mean that the very same formal proof text
works even if the statements of the invoked lemmas change.
This is often achieved by making proof commands performs some automatic
search that can compensate for the changes.
The concept of a robust proof is surely tantalizing but we
believe it is also a double-edge sword: The more
sophisticated robustness is, the harder it can get to identify the
root cause of the problem when things break (and sometimes they do, no
matter how robust things are).  On the
contrary the failure of a predictable, dumb, tool is easy
to understand, if only because it is simpler and less
sophisticated.\\
The tenet of the \mcbSSR{} proof language is that
predictable failure leads to easy to repair proofs.

As a consequence most \mcbSSR{} tactics fail early and locally: If they cannot
operate as expected, they fail rather than silently perform a no
operation or try to be smart and guess what one may have wanted to
do.  This helps errors to be discovered early in the proof script and
hence localize precisely where things actually broke. It is a matter
of style and discipline to preserve this property when composing
\mcbSSR{} tactics.  Indeed it is sometimes desirable to relax this
behavior, but it has to be done with care.

For example by applying the \C{?}
modifier in rewrite rules one can make a rewrite step never fail.
While this is very useful in practice,
abusing this feature typically results in scripts that are hard to
repair. The recurrent idiom is ``\C{rewrite lem1 ?lem2 // lem3}''.
Here \C{lem2} is used to trivialize a side condition of \C{lem1} that
is then closed by \C{//}. The \C{lem3} applies to the main goal
after the rewriting of \C{lem1}, and the next line is going to take
on from that point.
Now imagine what happens if the side condition of \C{lem1} changes and
\C{lem2} is not helping anymore in trivializing it and hence
\C{//} cannot solve it. In this scenario \C{lem3} is likely to fail,
since it is meant to apply to the main goal, and not the side
condition.\\
Writing ``\C{rewrite lem1 ?lem2 ?lem3 //}'' can have the same meaning but
is harder to repair. In particular one would discover the breakage
later, in the next line, since after \C{lem1} all actions never fail.
Also, the next line is likely is going to be run on the side
condition, and not the main goal. The person repairing the proof is
hence put on the wrong track: the line that fails was not even
written to operate on the goal she sees.

\subsection{Checkpointing}

Forward steps in large proofs play the role of checkpoints:
when a proof breaks one can start replaying it from the closest
forward step since it is granted that the goal is the very same
the original author was seeing when he wrote the proof. If there are
no checkpoints one may be forced to replay the proof from the very
beginning.

A recurrent idiom in the \mcbMC{} library is the one of refining an
assumption by replacing it with a stronger one. For example to refine
\C{H} one can write \C{have \{H\}H : ty}, or even \C{have
\{\}H : ty} in recent versions of the \mcbSSR{} language.

A related practice is the one of explicitly clearing hypothesis when
they are not needed anymore. The first beneficial effect is that the
context as displayed by \Coq{} becomes smaller, easier to read.  Also,
by making the lifetime of context entries explicit, one can more
easily read the dependencies among blocks, and that is a useful piece
of information when one refactors proofs.  To help keeping the context
tidy, most tactics clear their arguments automatically, unless told
otherwise. For example \C{apply: H} also removes \C{H} from the
context; similarly \C{move: H} or \C{case: H}.

\subsection{Large, monolithic, proofs: a fact of life}

Somehow counter-intuitively, large proofs cannot always be broken down
into smaller lemmas. In fact, chopping up long proofs in an
artificial manner is often counterproductive.

In particular, as contexts grow larger, gathering numerous facts and
local definitions, it becomes harder to extract an intermediate result
and state it outside the proof mode of the main result. Indeed, the
statement of such an auxiliary lemma would probably need to include a
large chunk of the big context, in order to remain provable. Besides
the trouble of copy-pasting this portion of the context, a lemma with
too many hypotheses soon becomes very hard to use, as instantiating
the latter becomes a technical issue. Lastly, it is often quite
difficult to find an appropriate name for such artificial lemmas.

The \C{pose} and \C{set} tactics provide facilities for stating local
definitions in proof mode. In particular, the \C{set} tactic eases the
definition of an abbreviation for a given subterm of the current goal,
by allowing to describe the latter via a pattern. 
The \C{have} tactic, and its variants \C{suffices} and \C{wlog},
provide syntactic facilities to state and use local definitions and
lemmas to a main proofs, with similar features as the \C{Lemma}
vernacular command (and its synonyms), like the automated introduction
of named parameters. The \C{wlog} tactic goes one step further, and
allows one to describe the verbose cut statements hidden behind a
\emph{without loss of generality} proof pattern by providing only the
names of the hypotheses in the context which should be included in the
generalization. This feature saves the user from an painful manual
writing of the explicit cut statement.

\subsection{On the usability of lemmas}

The usability of lemmas contributes to maintenance only indirectly,
that is by making proofs shorter. Still it plays a crucial role
in a library so it is worth spending a few words on it.

Most of the lemmas in the \mcbMC{} libraries are equalities, meant to be
used with the \C{rewrite} tactic. Let's take for example \C{(mul0n : forall n, 0 * n = 0)}. Two equivalent lemmas, mathematically speaking, are
\C{(mulnm_n0 : forall n m, n = 0 -> n * m = 0)}
and 
\C{(mul0_0n : forall n, 0 = 0 * n)}.

Let's see why \C{mul0n}, the one present in the library, is easier to use.
The \C{rewrite} tactic uses the left hand side of the conclusion as
a pattern and looks for subterms of the goal that match.
The pattern
provided by the \C{mul0n} is \C{(0 * \_)} and is quite precise, only the
right hand side of the multiplication is unknown. Moreover the presence of
\C{0} is very related to the meaning of the lemma, that shows it is
the annihilator of multiplication.

The pattern for
\C{mulnm_n0} is, on the contrary, the very imprecise \C{(\_ * \_)}.
This pattern is matched by any multiplication in the goal, including
the ones where the left hand side is not (provably) zero. As a consequence
one would need to drive \C{rewrite} manually, either by providing
a more specific pattern or by specializing the lemma passing to it
some arguments.

The pattern for \C{mul0_0n} is \C{0}. While it is very precise
\C{rewrite} does not know what to put in the resulting goal, that is
it cannot find out what \C{n} is by just matching the pattern.
As a consequence this argument has to be passed explicitly.
Of course \C{mul0_0n} can be used right-to-left, but it requires the
\C{$~$-}
switch to \C{rewrite}. In some sense its ``default'' is wrong.

By stating equalities with the most informative term on the left one
obtains lemmas that easier to rewrite with, since their
arguments and the subterm of the goal to be replaced are going to be
found, automatically, by pattern matching.

A similar line of reasoning can be applied to other tactics as well. For
example \C{apply: lem} infers some of the arguments to \C{lem} by unifying the
conclusion of its statement with the goal. The position of the arguments that
are not inferable this way can be chosen so that it is easy to pass them
explicitly.  For example \C{(leq_trans : forall y x z, x <= y -> y <= z -> x
<= z)} quantifies first the variable that cannot be inferred by unifying the
conclusion with the goal (since it does not occur in the conclusion).
It is hence sufficient to write \C{(leq_trans m)} to specify the only
missing piece of information.

\subsection{Qed is not the shift bell}

When a proof is finally accepted by \Coq{} it just means that it is
correct---the absence of mistakes does not imply readability or maintainability.

As we have argued so far, a proof is eventually going to break, and repairing a
tidy proof is much easier than repairing a messy one.
The advantage we have compared to, say, cleaning up regular computer code, is
that once a proof is complete and correct we can iterate small cleanup steps
much faster. Indeed we can have \Coq{} check the proof after each and every
step: If it's accepted, we introduced no errors and can move on with the next
cleanup iteration.

Unfortunately little support is given to the user to tidy proofs up by today's
user interfaces, even for very recurrent steps.  Among all, one that is very
frequent is the removal of duplication within the current proof or across the
entire library.  It is quite frequent to end up re-proving, inline, an existing
lemma when one does not know the lemma exists (or does not find it).

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \section{Exercises}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{Exercise}[label=ex:have,difficulty=0,title={Have}]
% 
% Prove the following lemma
% 
% \begin{coq}{name=ex1}{}
% Lemma maxn_idPl {m n} : n <= m -> maxn m n = m.
% \end{coq}
% 
% following this chain of forward steps
% 
% \begin{align*}
% n <= m &= n - m == 0 \\
%        &= m + (n - m) == m + 0 \\
%        &= m + (n - m) == m \\
%        &= maxn~ m~ n == m 
% \end{align*}
% 
% \end{Exercise}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Solutions}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{Answer}[ref=ex:have]
% 
% \begin{coq}{}{}
% Lemma maxn_idPl {m n} : n <= m -> maxn m n == m.
% Proof.
% move=> leq_nm.
% have {leq_nm} sub_nm : n - m == 0.
%   by apply: leq_nm.
% have {sub_nm} add_m_sub_nm : m + (n - m) == m + 0.
%   by rewrite eqn_add2l.
% suffices: m + (n - m) == m.
%   by rewrite maxnE.
% by rewrite -[X in _ == X]addn0.
% Qed.
% \end{coq}
% 
% \end{Answer}
% 
