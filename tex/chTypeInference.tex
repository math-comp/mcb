% vim:set tw=70:
% vim:set spell:
% vim:set errorformat="":
\begin{coqdef}{name=ssr}
Require Import Ssreflect.ssreflect.
From Ssreflect Require Import ssrfun ssrbool ssrnat eqtype fintype seq div prime.
Set Implicit Arguments.
Unset Strict Implicit.
Unset Printing Implicit Defensive.
\end{coqdef}
\begin{coqdef}{name=abort}
Abort.
\end{coqdef}

\chapter{Implicit parameters}{}

The rules of the \mcbCIC{}, as the ones sketched in~\ref{ch:ttch},
are expressed on the syntax of terms and
are implemented by the kernel of \Coq{}.  Such a software component
performs \emph{type checking}: given a term and type, it checks if such
term has the given type.  To keep type checking simple and decidable,
the syntax of terms makes all information explicit. As a consequence,
the terms written in such verbose syntax are pretty large.

Luckily the user very rarely interacts directly with the kernel.
Instead she almost always interacts with the refiner, a software
component that is able to accept open terms.  Open terms are in
general way smaller than regular terms because some information can be
left implicit~\cite{Pollack92implicitsyntax}.
In particular one can omit any subterm by writing
``\lstinline/_/'' in place of it.
Each missing piece of information is either reconstructed
automatically by the \emph{type inference} algorithm, or provided
interactively by means of proof commands.  In this chapter we
focus on type inference.

Type inference is \emph{ubiquitous}: whenever the user inputs a term
(or a type) the system tries to infer a type for it.
One can think of the work of the type inference
algorithm as trying to give a meaning to the input of the
user possibly completing and constraining it by inferring some
information.  If the algorithm succeeds, the term is accepted;
otherwise an error is given.

What is crucial to the \mcbMC{} library is that
\emph{the type inference algorithm is programmable}:
one can extend the
basic algorithm with small declarative programs\footnote{A program is
said to be declarative when it explains what it computes rather than
how. The programs in question are strictly linked with the Prolog
programming language, a technology that found applications in artificial
intelligence and computational linguistics.}
that have access to
%\marginnote{explain declarative programs}
the library of already formalized facts.  In this way one can make the
type inference algorithm aware of the contents of the library and
make \Coq{} behave as a trained reader who is able to guess the
intended meaning of a mathematical expressions from the context
thanks to her background knowledge.

This chapter also introduces the key concepts of \emph{interface}
and \emph{instance}.  An interface is essentially the signature
of an algebraic structure: operations, properties and notations
letting one reason abstractly about a family of objects sharing
the interface.
An instance is an example of an algebraic structure,
an object that fits an interface.
For example \C{eqType} is the interface of
data types that come equipped with a comparison function, and
the type \C{nat} forms, together with the \C{eqn} function, an
example of \C{eqType}.

The programs we will write to
extend type inference play two roles.  On one hand they link
instances to interfaces, like \C{nat} to \C{eqType}.
On the other hand they build \emph{derived
instances} out of basic ones.  For example we teach type inference
how to synthesize an instance of \C{eqType} for a type like
\C{(A * B)} whenever \C{A} and \C{B} are instances of \C{eqType}.

The concepts of interface and instances are recurrent in
both computer science and modern mathematics, but are not a primitive
notion in \Coq{}.  Despite that, they can be encoded quite naturally,
although not trivially, using inductive types and the dependent function
space.   This encoding is not completely orthogonal to the
actual technology (the type inference and its extension mechanism).
For this reason we shall need to dive, from time to time, into
technical details, especially in sections labelled with two stars.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbLEARN{HO unif is hard}
%\mcbREQUIRE{}
%\mcbPROVIDE{terminology}
\section{Type inference and higher-order unification}\label{sec:hounif}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The type inference algorithm is quite similar to the type checking
one: it recursively traverses a term checking that each subterm has a
type compatible with the type expected by its context.  During type
checking types are compared taking computation into account.  Types
that compare as equal are said to be \emph{convertible}.
Termination of reduction and uniqueness of normal forms provide
guidance for implementing the convertibility test, for which a
complete and sound algorithm exists.  Unfortunately type
inference works on open terms, and this fact turns convertibility into
a much harder problem called \emph{higher-order unification}.  The
special placeholder ``\lstinline/_/'', usually called \emph{implicit
argument}, may occur inside types and stands for one, and only one,
term that is not explicitly given.  Type inference does not check if
two types are convertible; it checks if they unify.
Unification is allowed to assign values to implicit arguments in order
to make the resulting terms convertible.  For example unification is
expected to find an assignment that makes the type
\lstinline/(list _)/ convertible to \lstinline/(list nat)/.
By picking the value \lstinline/nat/ for the placeholder
the two types become syntactically equal and hence convertible.
% Unification shall fix the value of an implicit argument only if it
% is strictly needed.

Unfortunately it is not hard to come up with classes of examples where
guessing appropriate values for implicit arguments is, in general, not
possible. In fact, such guessing has been shown to be as hard as proof
search in the presence of higher-order constructs.
For example to unify \lstinline/(prime _)/ with
\lstinline/true/ one has to guess a prime number. Remember that
\lstinline/prime/ is a boolean function that fed with a natural
number returns either \lstinline/true/ or \lstinline/false/.
While assigning \lstinline/2/ to the implicit argument
would be a perfectly valid solution, it is clear
that it is not the only one.  Enumerating all possible
values until one finds a valid one is not a good strategy
either, since the good value may not exist.  Just think at the
problem \lstinline/(prime (4 * _))/ versus \lstinline/true/.  An even
harder class of problems is the one of synthesizing programs.
Take for example the unification problem \lstinline/(_ 17)/ versus
\lstinline/[:: 17]/.  Is the function we are looking for the list
constructor? Or maybe, is it a factorization algorithm?

Given that there is no silver bullet for higher-order unification
\Coq{} makes a sensible design choice: provide an (almost)
heuristic-free algorithm and let the user extend it via an extension
language.  We refer to such language as the language of
\emph{canonical structures}.  Despite being a very restrictive language,
it is sufficient to program a wide panel of useful functionalities.

The concrete syntax for implicit arguments, an underscore character,
does not let one name the missing piece of information.
If an expression contains multiple occurrences
of the placeholder ``\lstinline/_/'', they are all considered as
potentially different by the system, and hence hold (internally)
unique names.  For the sake of clarity we take the freedom to
use the alternative syntax \mcbimpl{x} for implicit arguments (where
$x$ is a unique name).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbLEARN{type and term inference}
%\mcbREQUIRE{have, move, exact}
%\mcbPROVIDE{Arguments (setting implicit)}
\section{Type inference by example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Most of the code we have discussed so far uses some amount of type
inference. In this section, we go over a few elementary examples, so
as to clarify them.

For once, let us start with an example of code meant to be executed in
a \Coq{} session before \emph{importing any library, nor positioning any
global option}. This example is one of the simplest possible: it
defines the polymorphic identity function and checks its application to
\lstinline/3/.

\begin{coq-left}{name=exid}{width=8cm,title=Polymorphic identity}
Definition id A (a : A) : A := a.
Check (id nat 3).
Check (id _ 3).
\end{coq-left}
\coqrun{name=r1}{exid}
\begin{coqout-right}{run=r1}{title=Response,width=4cm}
$~$
id nat 3 : nat
id nat 3 : nat
\end{coqout-right}

In the expression \lstinline/(id nat 3)/ no subterm was omitted,
therefore \Coq{} accepts the term and prints its type.  In the third
line, even if the subterm \lstinline/nat/ is omitted, \Coq{} accepts
the term.  Type inference finds a value for the placeholder
for the user by proceeding in the following way:  it traverses the term
recursively from left to right, ensures that the type of each
argument of the application had the type expected by the function.  In
particular \lstinline/id/ takes two arguments.
The former argument is expected to have type \lstinline/Type/ and the
user leaves such argument implicit (we name it \mcbimpl{A}).   Type
inference imposes that \mcbimpl{A} has type \lstinline/Type/, and this
constraint is satisfiable.  The algorithm continues checking the
remaining argument.  According to the definition of \lstinline/id/ the type of
the second argument must be the value of the first argument.  Hence
type inference runs recursively on the argument \lstinline/3/
discovering it has type \lstinline/nat/ and imposes that it unifies
with the value of the first argument (that is \mcbimpl{A}).  For this
to be true \mcbimpl{A} has to be assigned the value \lstinline/nat/.
As a result the system prints the input term, where the placeholder
has been replaced by the value type inference assigned to it.

In the light of this process, it becomes clear that every time we
apply the identity function to a term, we can omit to specify its
first argument, since \Coq{} is able to infer it and complete the
input term for us.  This phenomenon is so common that one can ask
the system to insert the right number of \lstinline/_/ for us. Here
we only provide a simple example. For more details
% see section~\ref{sec:declaringimpl} or
refer to~\cite[``Extensions of Gallina'']{Coq:manual}.  

\begin{coq-left}{name=impl-arg-id}{title=Setting implicit arguments,width=6cm}
Arguments id {A} a.
Check (id 3).
Check (@id nat 3).
\end{coq-left}
\coqrun{name=iarg}{impl-arg-id}
\begin{coqout-right}{run=iarg}{title=Response,width=6cm}
$~$
id 3 : nat
id 3 : nat
\end{coqout-right}

The \lstinline/Arguments/ directive ``documents'' the constant
\lstinline/id/.  In this case it just marks the argument that has to
be considered as implicit by surrounding it with curly braces. In our
case, argument \C{A} is thus declared as implicit.
The declaration of implicit arguments can be locally disabled by
prefixing the name of the constant with the \lstinline/@/ symbol.

Very often, it is possible to determine the most plausible status
of each argument, explicit or implicit, using a heuristic. Such a
heuristic is applied in a systematic way when a global option is
declared: every definition posterior to the declaration may feature
implicit arguments even if not \C{Argument} command is used to declare
them manually. This has been the case so far, and in the rest of the
section and of the book, we will work again under the assumption that
following the global options have been set, in accordance with
Chapter~\ref{ssec:runcoq}:

\begin{coq}
Set Implicit Arguments.
Unset Strict Implicit.
Unset Printing Implicit Defensive.
\end{coq}

Of course, the \C{Argument} command remains useful in specific cases
when a finer, manual tuning of implicit arguments is needed.
The command \C{About} provides the user with the type of
a given constant, and with the status, implicit or not, of each
argument of the constant.

Another piece of information that is often not explicit is
the type of abstracted or quantified variables.

\begin{coq-left}{name=infarg}{title=Omitting type annotations,width=7cm}
Check (fun x => @id nat x).
$~$
Lemma prime_gt1 p :
  prime p -> 1 < p.
$~$
\end{coq-left}
\coqrun{name=inf}{ssr,infarg,abort}
\begin{coqout-right}{run=inf}{title=Response,width=5cm}
fun x : nat => id x : nat -> nat
$~$
p : nat
============================
 prime p -> 1 < p
\end{coqout-right}

In the first line the syntax (\lstinline/fun x => ...)/ is sugar for
\lstinline/(fun x : _ => ...)/ where we leave the type of
\lstinline/x/ implicit.  Type inference fixes it to \lstinline/nat/
when it reaches the last argument of the identity function.
It unifies the type of \lstinline/x/ with the value of the first
argument given to \lstinline/id/ that in this case is \lstinline/nat/.
This last example is emblematic: most of the time the type of
abstracted variables can be inferred by looking at how they are used.
This is very common in lemma statements.  For example, the third line
states a theorem on \lstinline/p/ without explicitly giving its type.
Since the statement uses \lstinline/p/ as the argument of the
\lstinline/prime/ predicate, it is automatically constrained to be
of type \lstinline/nat/.

The kind of information filled in by type inference can also be of
another, more interesting, nature.  So far all placeholders were
standing for types, but the user is also allowed to put \lstinline/_/
in place of a term.

\begin{coqdef}{name=infdata}
Lemma example q : prime q -> 0 < q.
Proof.
move=> pr_q. Redirect "g1" Show.
have q_gt1 := @prime_gt1 _ pr_q.
exact: ltnW q_gt1.
Qed.
\end{coqdef}
\begin{coq-left}{def=infdata}{title=Inferring a term,width=7cm}
Lemma example q : prime q -> 0 < q.
Proof.
move=> pr_q.
\end{coq-left}
\begin{coqout-right}{run=inf2;out=g1}{title=Goal after line 3,width=5cm}
q : nat
pr_q : prime q
========================
0 < q
\end{coqout-right}

The proof begins by giving the name \lstinline/pr_q/ to the assumption
\lstinline/(prime q)/. 
Then it builds a proof term by hand using
the lemma stated in the previous example and names it \lstinline/q_gt1/.

\begin{coq}{}{}
have q_gt1 := @prime_gt1 _ pr_q.
exact: ltnW q_gt1.
Qed.
\end{coq}

In the expression \lstinline/(prime_gt1 _ pr_q)/, the placeholder,
that we name \mcbimpl{p}, stands for a natural number.
When type inference reaches \mcbimpl{p}, it fixes its type to \lstinline/nat/.
What is more interesting is what happens when type inference reaches the
\lstinline/pr_q/ term.  Such term has its type fixed by the context:
\lstinline/(prime q)/.  The type of the second argument expected by
\lstinline/prime_gt1/ is \lstinline/(prime $\mcbimplm{p}$)/ (i.e., the
type of \lstinline/prime_gt1/ where we substitute \mcbimpl{p} for
\lstinline/p/.  Unifying \lstinline/(prime $\mcbimplm{p}$)/ with
\lstinline/(prime q)/ is possible by assigning \lstinline/q/ to
\mcbimpl{p}.  Hence the proof term just constructed is
well typed, its type is \lstinline/(1 < q)/ and the placeholder
has been set to be \lstinline{q}.
As we did for the identity function, we can declare the \lstinline/p/
argument of \lstinline/prime_gt1/ as implicit: this is actually the
case in the library. In this case, the script is:

\begin{coq}{}{}
have q_gt1 := prime_gt1 pr_q.
exact: ltnW q_gt1.
Qed.
\end{coq}

Choosing a good  declaration of implicit arguments for lemmas is
tricky and requires one to think ahead how the lemma is used.
% section~\ref{sec:declaringimpl} is dedicated to that.

So far we have been using only the simplest form of type inference in
our interaction with the system. The unification problems we have
encountered would have been solved by a first order unification
algorithm and we did not need to compute or synthesize
\emph{functions}. In the next section we illustrate how the
unification algorithm used in type inference can be extended in order
to deal with higher-order problems. This extension is based on the use
of declarative programs, and we present the encoding of the relations
which describe these programs.
As of today however there is no precise, published, documentation of the type
inference and unification algorithms implemented in \Coq{}.  For a
technical presentation of a type inference algorithm close enough to
the one of \Coq{} we suggest the interested reader to
consult~\cite{DBLP:journals/corr/abs-1202-4905}.  The reader
interested in a technical presentation of a simplified version of the
unification algorithm implemented in \Coq{} can
read~\cite{unifcoq,betaderekjournal}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbLEARN{records as relations, canonical base instances}
%\mcbREQUIRE{}
%\mcbPROVIDE{Canonical}
%\mcbNOTES{}
\section{Records as relations}\label{sec:eqtype}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In computer science a record is a very common data structure.  It is a
compound data type, a container with named fields.  Records are
represented faithfully in the \mcbCIC{} as
inductive data types with just one constructor,
recall section~\ref{sec:records}.
The peculiarity of the records we are going to use is that they are
\emph{dependently typed}: the type of each field is allowed to depend on
the values of the fields that precede it.

\Coq{} provides syntactic sugar for declaring record types.

\begin{coq}{name=eqtype}{}
Record eqType : Type := Pack {
  sort : Type;
  eq_op : sort -> sort -> bool
}.
\end{coq}
\label{eqtype:noproof}
\coqrun{name=eqtype1}{ssr,eqtype}

The sentence above declares a new inductive type called
\lstinline/eqType/ with one constructor named
\lstinline/Pack/ with two arguments.  The first one
is named \lstinline/sort/ and holds a type; the second and last
one is called \lstinline/eq_op/ and holds a comparison function
on terms of type \lstinline/sort/.  We recall that what this special syntax
does is declaring at once the following inductive type plus
a named projection for each record field:

\begin{coq}{name=eqtype2}{}
Inductive eqType : Type :=
  Pack sort of sort -> sort -> bool.
Definition sort (c : eqType) : Type :=
  let: Pack t _ := c in t.
Definition eq_op (c : eqType) : sort c -> sort c -> bool :=
  let: Pack _ f := c in f.
\end{coq}
\coqrun{name=eqtype2}{ssr,eqtype2}

Note that the type dependency between the two fields requires the first
projection to be used in order to define the type of the second projection.

We think of the \lstinline/eqType/ record type as a \emph{relation}
linking a
data type with a comparison function on that data type.  Before
putting the \lstinline/eqType/ relation to good use we declare an
inhabitant of such type, that we call an \emph{instance}, and we
examine a crucial property of the two projections just defined.

We relate the \C{eqn} comparison function with the \lstinline/nat/
data type.

\begin{coq}{name=eqn}{}
Definition nat_eqType : eqType := @Pack nat eqn.
\end{coq}
\coqrun{name=eqn}{ssr,eqtype,eqn}

Projections, when applied to a record instance like
\lstinline/nat_eqType/ compute and extract the desired component.

\begin{coq-left}{name=redproj}{title=Computation of projections,width=6cm}
Eval simpl in sort nat_eqType.
Eval simpl in @eq_op nat_eqType.
\end{coq-left}
\coqrun{name=r}{ssr,eqtype,eqn,redproj}
\begin{coqout-right}{run=r}{title=Response,width=6cm}
= nat : Type
= eqn : sort nat_eqType ->
         sort nat_eqType -> bool
\end{coqout-right}

Given that \lstinline/(sort nat_eqType)/ and \lstinline/nat/
are convertible, equal up to computation, we can use the two terms
interchangeably.  The same holds for \lstinline/(eq_op nat_eqType)/
and \lstinline/eqn/.  Thanks to this fact \Coq{} can type check the
following term:

\begin{coq-left}{name=eqop}{width=5.7cm}
Check (@eq_op nat_eqType 3 4).
\end{coq-left}
\coqrun{name=e1}{ssr,eqtype,eqn,eqop}
\begin{coqout-right}{run=e1}{width=6.3cm}
eq_op 3 4 : bool
\end{coqout-right}

This term is well typed, but checking it is not as simple as one may
expect.
The \lstinline/eq_op/ function is applied to three arguments.
The first one is \lstinline/nat_eqType/ and its type,
\lstinline/eqType/, is trivially equal to the one expected by
\lstinline/eq_op/.
The following two arguments are hence expected to be of type
\lstinline/(sort nat_eqType)/ but \lstinline/3/ and \lstinline/4/ are
of type \lstinline/nat/.
Recall that unification takes computation into account exactly as the
convertibility relation.  In this case the unification algorithm
unfolds the definition of \lstinline/nat_eqType/ obtaining
\lstinline/(Pack nat eqn)/ and reduces the projection
extracting  \lstinline/nat/.  The obtained term literally matches the
type of the last two arguments given to \lstinline/eq_op/.

Now, why this complication?  Why should one prefer
\lstinline/(eq_op nat_eqType 3 4)/ to \lstinline/(eqn 3 4)/?
The answer is \emph{overloading}.
It is recurrent in mathematics and computer science to reuse
a symbol, a notation, in two different contexts.  A typical
example coming from the mathematical practice is to use the same
infix symbol $*$ to denote any ring multiplication.  A typical
computer science example is the use of the same infix
\lstinline/==/ symbol to denote the comparison over any data type.
Of course the underlying operation one intends to use depends on
the values it is applied to, or better their type
\footnote{The meaning of a symbol in mathematics is even deeper: by
	writing $a * b$ one may expect the reader to figure out which
	ring she talks about, recall its theory, and use this
	knowledge to justify some steps in a proof.  By programming type
	inference appropriately, we model this practice
in section~\ref{rec:itf}.}.
Using records lets us model these practices.
Note that, thanks to its higher-order nature, the term \lstinline/eq_op/
can always be the head symbol denoting a comparison.  This makes
it possible to recognize, hence print, comparisons in a uniform way
as well as to input them.  On the contrary, in the simpler expression
\lstinline/(eqn 3 4)/, the name of the head symbol is very specific to
the type of the objects we are comparing.

% Also note that
% \marginnote{A bit too technical and boring}
% polymorphism, in the sense of the \lstinline/ML/ programming language,
% is not what we are looking for, since it would impose the comparison
% function to behave uniformly on every type.  What we are looking
% for is closer to the ad-hoc polymorphism of the \lstinline/Haskell/
% programming language or the notion of subtyping provided by object
% oriented languages.

In the rest of this chapter, we focus on the overloading of the
\lstinline/==/ symbol and we start by defining another comparison
function, this time for the \lstinline/bool/ data type.

\begin{coq}{name=bety}{}
Definition eqb (a b : bool) := if a then b else ~~ b.
Definition bool_eqType : eqType := @Pack bool eqb.
\end{coq}

Now the idea is to define a notation that applies to any occurrence
of the \lstinline/eq_op/ head constant and use such
notation for both printing and parsing.

\begin{coqdef}{name=infix}
Notation "x == y" := (@eq_op _ x y).
\end{coqdef}
\begin{coqdef}{name=print}
Check (@eq_op bool_eqType true false).
Check (@eq_op nat_eqType 3 4).
\end{coqdef}
\begin{coq-left}{name=infix,print}{title=Overloaded notation,width=7cm}
Notation "x == y" := (@eq_op _ x y).
Check (@eq_op bool_eqType true false).
Check (@eq_op nat_eqType 3 4).
\end{coq-left}
\coqrun{name=r2}{ssr,eqtype,eqn,bety,infix,print}
\begin{coqout-right}{run=r2}{title=Response,width=5cm}
$~$
true == false : bool
3 == 4 : bool
\end{coqout-right}

As a printing rule, the placeholder stands for a wild card: the
notation is used no matter the value of the first argument of
\lstinline/eq_op/.  As a result both occurrences of \lstinline/eq_op/,
line 2 and 3, are printed using the infix \lstinline/==/ syntax.
Of course the two operations are different; they are specific to the
type of the arguments and the typing discipline ensures the
arguments match the type of the comparison function packaged in
the record.

When the notation is used as a parsing rule, the placeholder is
interpreted as an implicit argument: type inference is expected to find a value
for it.  Unfortunately such notation does not work as a parsing rule
yet.

\begin{coq-left}{name=parsenocs}{width=4cm}
Check (3 == 4).
$~$
\end{coq-left}
\begin{coqout-right}{run=nc}{width=8cm}
Error: The term "3" has type "nat" while
it is expected to have type "sort ?e".
\end{coqout-right}
\coqrun{name=nc;fail}{ssr,eqtype,eqn,infix,parsenocs}

If we unravel the notation, the input term is really
\lstinline/(eq_op _ 3 4)/. We name the placeholder \mcbimpl{e}.
If we replay the type inference steps seen before, the unification
step is now failing.  Instead of \lstinline/(sort nat_eqType)/
versus \lstinline/nat/, now unification has to solve the problem
\lstinline/(sort $\mcbimplm{e}$)/ versus \lstinline/nat/.
This problem falls in one of the problematic classes we presented in
section~\ref{sec:hounif}: the system has to synthesize a comparison
function (or better a record instance containing a comparison
function).

\Coq{} gives up, leaving to the user the task of extending the
unification algorithm with a declarative program that is able to solve
unification problems of the form \lstinline/(sort $\mcbimplm{e}$)/,
versus \lstinline/T/ for any \lstinline/T/.
Given the current context, it seems reasonable to write an
extension that picks \lstinline/nat_eqType/ when \lstinline/T/ is
\lstinline/nat/ and \lstinline/bool_eqType/ when \lstinline/T/ is
\lstinline/bool/.  In the language of \C{Canonical Structure}s, such
a program is expressed as follows.

\begin{coq}{name=declcs}{title=Declaring canonical structures}
Canonical nat_eqType.
Canonical bool_eqType.
\end{coq}

The keyword \lstinline/Canonical/ was chosen to stress that the
program is deterministic: each type  \lstinline/T/ is related to
(at most) one \emph{canonical} comparison function.

\begin{coq-left}{name=parse}{width=6cm}
Check (3 == 4).
Check (true == false).
Compute (3 == 4).
\end{coq-left}
\coqrun{name=cs}{ssr,eqtype,eqn,bety,infix,declcs,parse}
\begin{coqout-right}{run=cs}{width=6cm}
3 == 4 : bool
true == false : bool
= false : bool
\end{coqout-right}

The mechanics of the small program we wrote using the
\lstinline/Canonical/ keyword can be explained using the
global table of canonical solutions.
Whenever a record instance is declared as canonical \Coq{}
adds to such table an entry for each field of the record type.

\vspace{1ex}
\begin{tcolorbox}[colframe=orange!60!white,before=\hfill,after=\hfill,width=8cm,center title,tabularx={ll|l},fonttitle=\sffamily\bfseries,title=canonical structures Index]
projection & value & solution \\ \hline
\lstinline/sort/ & \lstinline/nat/ & \lstinline/nat_eqType/  \\
\lstinline/sort/ & \lstinline/bool/ & \lstinline/bool_eqType/   \\
%\lstinline/eq_op/ & \lstinline/eqn/ & \lstinline/nat_eqType/  \\
%\lstinline/eq_op/ & \lstinline/eqb/ & \lstinline/bool_eqType/   \\
\hline
\end{tcolorbox}

Whenever a unification problem with the following shape is encountered,
the table of canonical solution is consulted.
\begin{center}
\lstinline/(projection $\mcbimplm{S}$)/ ~~versus~~ \lstinline/value/
\end{center}
The table is looked up using as keys the projection name and the
value.  The corresponding solution is assigned to the implicit
argument \mcbimpl{S}.

In the table we reported only the relevant entries.  Entries
corresponding to the \lstinline/eq_op/ projection play no role
in the \mcbMC{} library. The names of such projections are
usually omitted to signal that fact.

What makes this approach interesting for a large library is that
record types can play the role of interfaces.  Once a record type has
been defined and some functionality associated to it, like a notation,
one can easily hook a new concept up by defining a corresponding
record instance and declaring it canonical.  One gets immediately all
the functionalities tied to such interface to work on the new concept.
For example a user defining new data type with a comparison function
can immediately take advantage of the overloaded \lstinline/==/
notation by packing the type and the comparison function in an
\lstinline/eqType/ instance.

This pattern is so widespread and important that the \mcbMC{}
library
consistently uses the synonym keyword \lstinline/Structure/ in place of
\lstinline/Record/ in order to make record types playing the role
of interfaces easily recognizable.
\index[vernac]{\C{Structure}}

The computer-science inclined reader shall see records as first-class
values in the \mcbCIC{} programming language. In other words,
the projections of a record are just ordinary functions, defined by
pattern-matching on an inductive type, and which access the fields
of the record.
% Exercise~\ref{ex:triple} proposed to implement the
% projections of a triple onto its components, and these functions are
% the exact analogues of the projections of a record with three fields.
In particular,  the fields of two given instances of records can be
combined and used to build a new instance of another record. Canonical
structures provide a language to describe how new instances of
records, also called structures in this case, can
be built from existing ones, via a set of combinators defined by the
user. %This is the subject of the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \mcbLEARN{derived instances}
% \mcbREQUIRE{Canonical}
% \mcbPROVIDE{RecCanonical}
% \mcbsection{Records as first-class relations}\label{sec:receqtype}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

So far we have used the \lstinline/==/ symbol for terms whose type is
atomic, like \lstinline/nat/ or \lstinline/bool/.  If we try for
example to use it on terms whose type was built using a type
constructor like the one of pairs we encounter an error.

\begin{coq-left}{name=reccs}{title=Error,width=6cm}
Check (3, true) == (4, false).
$~$
$~$
\end{coq-left}
\coqrun{name=nc1;fail}{ssr,eqtype,eqn,bety,infix,declcs,reccs}
\begin{coqout-right}{run=nc1}{title=Response,width=6cm}
Error: The term "(3, true)" has type
"(nat * bool)%type" while it is expected
to have type "sort ?e".
\end{coqout-right}

The term \lstinline/(3,true)/ has type \lstinline/(nat * bool)/ and,
so far, we only taught \Coq{} how to compare booleans and natural
numbers, not how to compare pairs.
Intuitively the way to compare pairs is to compare their components
\emph{using the appropriate comparison function}.
Let's write a comparison function for pairs.

\begin{coq}{name=paircs}{title=Comparing pairs}
Definition prod_cmp eqA eqB x y :=
  @eq_op eqA x.1 y.1 && @eq_op eqB x.2 y.2.
\end{coq}

What is interesting about this comparison function is that the
pairs \lstinline/x/ and \lstinline/y/ are not allowed to have
an arbitrary, product, type here.  The typing constraints imposed
by the two \lstinline/eq_op/ occurrences force the type of
\lstinline/x/ and \lstinline/y/ to be
\lstinline/(sort eqA * sort eqB)/.  This means
that the records \lstinline/eqA/ and \lstinline/eqB/ hold
a sensible comparison function for, respectively, terms of
type \lstinline/(sort eqA)/ and \lstinline/(sort eqB)/.

It is now sufficient to pack together the pair data type
constructor and this comparison function in an \lstinline/eqType/
instance to extend the canonical structures inference machinery
with a new combinator.

\begin{coq}{name=declcs2}{title=Recursive canonical structure}
Definition prod_eqType (eqA eqB : eqType) : eqType :=
  @Pack (sort eqA * sort eqB) (@prod_cmp eqA eqB).
Canonical prod_eqType.
\end{coq}

The global table of canonical solutions is extended as follows.

\vspace{1ex}
\noindent
\begin{tcolorbox}[colframe=orange!60!white,before=\hfill,after=\hfill,center title,tabularx={ll|l|l},fonttitle=\sffamily\bfseries,title=canonical structures Index]
projection & value & solution & combines solutions for \\ \hline
\lstinline/sort/ & \lstinline/nat/ & \lstinline/nat_eqType/ & \\
\lstinline/sort/ & \lstinline/bool/ & \lstinline/bool_eqType/ &  \\
\lstinline/sort/ & \lstinline/T1 * T2/ & \lstinline/prod_eqType pA pB/
	& \lstinline/pA/ $\leftarrow$ (\lstinline/sort/,\lstinline/T1/),
	  \lstinline/pB/ $\leftarrow$ (\lstinline/sort/,\lstinline/T2/)\\
\hline
\end{tcolorbox}

The third column is empty for base instances while it contains
the recursive calls for instance combinators.  With the updated
table, when the unification problem
\begin{center}
\lstinline/(sort $\mcbimplm{e}$)/ ~~versus~~ \lstinline/(T1 * T2)/
\end{center}
is encountered, a solution for \mcbimpl{e} is found by proceeding
in the following way.  Two new unification problems are generated:
\lstinline/(sort $\mcbimplm{eqA}$)/ versus \lstinline/T1/ and
\lstinline/(sort $\mcbimplm{eqB}$)/ versus \lstinline/T2/.  If both
are successful and \lstinline/v1/ is the solution for
\mcbimpl{eqA} and \lstinline/v2/ for \mcbimpl{eqB}, the solution for
\mcbimpl{e} is \lstinline/(prod_eqType v1 v2)/.

After the table of canonical solutions has been extended, our example
is accepted.

\begin{coq-left}{name=parsecs2}{width=6.3cm}
Check (3, true) == (4, false).
\end{coq-left}
\coqrun{name=cs2}{ssr,eqtype,eqn,bety,infix,declcs,paircs,declcs2,parsecs2}
\begin{coqout-right}{run=cs2}{width=5.7cm}
(3, true) == (4, false) : bool
\end{coqout-right}

The term synthesized by \Coq{} is the following one:

\begin{coq}{name=parsecs3}{}
   @eq_op (prod_eqType nat_eqType bool_eqType) (3, true) (4, false).
\end{coq}
\coqrun{name=cs3}{ssr,eqtype,eqn,bety,infix,declcs,paircs,declcs2,parsecs3}

% \marginnote{
% Make other examples? Other overloaded stuff: maybe and example of
% how to hook up to infix in ? or locked? or whatever?
% In any case a table with ``all'' the interfaces should probably be
% part of the book
% }
%
% In the running example of this chapter we use the canonical structures
% language to express structurally recursive programs on the syntax
% of types.  The \mcbCIC{} allows arbitrary terms to occur inside
% types.  As a consequence the language of canonical structures can
% express also structurally recursive programs on the syntax
% of terms.  This capability is used, for example, in
% section~\ref{sec:bigoplemmas} to related Monoid laws to function
% symbols to model the syntax and theory of iterated, ``big'', operators.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbREQUIRE{records + CS}
%\mcbPROVIDE{eqType}
%\mcbLEARN{interface to a theory}
\section{Records as (first-class) interfaces}\label{rec:itf}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When we define an overloaded notation, we convey
through it more than just the arity (or the type) of the associated
operation. We associate with it a property, or a collection thereof.
For example, in the context of group theory, the infix \C{+} symbol
is typically preferred to \C{*} whenever the group law is
commutative.

Going back to our running example, the actual definition of \lstinline/eqType/
used in the \mcbMC{} library also contains a property which enforces
the correctness and the completeness of the comparison test.

\begin{coq}{name=eqtype}{title=eqType}
Module Equality.

Structure type : Type := Pack {
  sort : Type;
  op : sort -> sort -> bool;
  axiom : forall x y, reflect (x = y) (op x y)
}.

End Equality.
\end{coq}

The extra property turns the
\lstinline/eqType/ relation into a proper \emph{interface},
which fully specifies what \C{op} is.

The axiom says that the boolean comparison function
is compatible with equality: two ground terms compare as equal if and
only if they are syntactically equal.  Note that this means that the
comparison function is not allowed to quotient the type by identifying
two syntactically different terms.

\mantra{The infix notation \lstinline/==/ stands for
a comparison function compatible with Leibniz equality
(substitution in any context).}

The \C{Equality} module enclosing the record acts as a name space:
\C{type}, \C{sort},
\C{op} and \C{axiom}, four very generic words, are here
made local to the \C{Equality} name space becoming, respectively,
\C{Equality.type}, \C{Equality.sort}, \C{Equality.op} and \C{Equality.axiom}.

As in section~\ref{sec:eqtype}, the record plays the role of a
relation and its \C{sort} component is again the only field that
drives canonical structure inference. The set of operations (and
properties) that define an interface is called a \emph{class}.  In the
next chapter, we are going to re-use already defined classes in order
to build new ones by mixing-in additional properties (typically called
axioms).  Hence the definition of \C{eqType} in the \mcbMC{} library
is closer to the following one:

\begin{coq}{name=eqtype}{title=The real definition of eqType}
Module Equality.

Definition axiom T (e : rel T) := forall x y, reflect (x = y) (e x y).

Record mixin_of T := Mixin {op : rel T; _ : axiom op}.
Notation class_of := mixin_of.

Structure type : Type := Pack {sort :> Type; class : class_of sort; }.

End Equality.

Notation eqType := Equality.type.
Definition eq_op T := Equality.op (Equality.class T).
Notation "x == y" := (@eq_op _ x y).
\end{coq}

In this simple case, there is only one property, named
\C{Equality.axiom}, and the class is exactly the mixin.

That being said, nothing really changes: the \C{eqType} structure
relates a type with a signature.

Remark the use of \C{:>} instead of \C{:} to type the
field called \C{sort}.  This tells \Coq{} to declare the
\C{Equality.sort} projection as a coercion. This makes it possible to
write \C{(forall T : eqType, forall x y : T, P)} even if \C{T} is not
a type and only \C{(sort T)} is.


\gotcha{Since \C{Equality.sort} is a coercion, it is not displayed by
\Coq{}; hence error messages about a missing canonical instance
declaration typically look very confusing, akin to: ``\ldots has type \C{nat}
but should have type \C{?e}'', instead of ``\ldots but should have
type \C{(sort ?e)}''. }

Given the new definition of \lstinline/eqType/,
when we write \lstinline/(a == b)/, type inference does not only infer
a function to compare \lstinline/a/ with \lstinline/b/, but also a
proof that such a function is correct.
Declaring the \C{eqType} instance for \C{nat} now requires some
extra work, namely proving the correctness of the \C{eqn} function.

\begin{coq}{name=eqtype}{}
Lemma eqnP : Equality.axiom eqn.
Proof.
move=> n m; apply: (iffP idP) => [|<-]; last by elim n.
by elim: n m => [|n IHn] [|m] //= /IHn->.
Qed.
\end{coq}

We now have all the pieces to declare \C{eqn} as canonical.

\begin{coq}{name=eqtype}{title=Making eqn canonical}
Definition nat_eqMixin := Equality.Mixin eqnP.
Canonical nat_eqType := @Equality.Pack nat nat_eqMixin.
\end{coq}

Note that the \C{Canonical} declaration is expanded (showing the
otherwise implicit first argument of \C{Pack}) to document that
we are relating the type \C{nat} with its comparison operation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbLEARN{overloaded lemmas}
%\mcbREQUIRE{}
%\mcbPROVIDE{eqP}
\section{Using a generic theory}\label{sec:eqtypetheory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The whole point of defining interfaces is to share a theory
among all examples of each interface.
In other words a theory proved starting from the
properties (axioms) of an interface applies to all its instances,
transparently.  Every lemma part of an abstract theory
is \emph{generic}: the very same name can be used for each and every
instance of the interface, exactly as the \C{==} notation.

The simplest lemma part of the theory of \C{eqType} is the
\lstinline/eqP/ generic lemma that can be used in conjunction with
any occurrence of the \lstinline/==/ notation.
\index[coq]{\C{eqP}}

\begin{coq}{name=eqP}{title=The eqP lemma}
Lemma eqP (T : eqType) : Equality.axiom (@Equality.op T).
Proof. by case: T => ty [op prop]; exact: prop. Qed.
\end{coq}

The proof is just unpacking the input \lstinline/T/.
We can use it on a concrete example of \C{eqType} like \C{nat}

\begin{coq}{name=test}{}
Lemma test (x y : nat) : x == y -> x + y == y + y.
Proof. by move=> /eqP ->. Qed.
\end{coq}
\coqrun{name=r7}{ssr,eqtype,eqP}
\coqrun{name=r8}{ssr,test}

In short, \C{eqP} can be used to change view: turn any
\C{==} into \C{=} and vice versa.

The \C{eqP} lemma also applies to abstract instances of \C{eqType}.
When we rework the instance of the type \C{(T1 * T2)} we see that the
proof, by means of the \C{eqP} lemma, uses the axiom of \C{T1} and
\C{T2}:

\begin{coq}{name=eqtype}{title=The complete definition of prod\_eqType}
Section ProdEqType.
Variable T1 T2 : eqType.

Definition pair_eq := [rel u v : T1 * T2 | (u.1 == v.1) && (u.2 == v.2)].

Lemma pair_eqP : Equality.axiom pair_eq.
Proof.
move=> [x1 x2] [y1 y2] /=; apply: (iffP andP) => [[]|[<- <-]] //=.
by move/eqP->; move/eqP->.
Qed.

Definition prod_eqMixin := Equality.Mixin pair_eqP.
Canonical prod_eqType := @Equality.Pack (T1 * T2) prod_eqMixin.
End ProdEqType.
\end{coq}
\index[coq]{\C{[pred x : T | E]}}
\index[coq]{\C{[rel x y : T | E]}}

where notation \C{[rel x y : T | E]} defines a binary boolean relation
on type \C{T}. Note that a similar notation \C{[pred x : T | E]}
exists for unary boolean predicates.

The generic lemma \C{eqP} applies to
any \C{eqType} instance, like \C{(bool * nat)}

\begin{coq}{name=test}{}
Lemma test (x y : nat) (a b : bool) : (a,x) == (b,y) -> fst (a,x) == b.
Proof. by move=> /eqP ->. Qed.
\end{coq}
\coqrun{name=r7}{ssr,eqtype,eqP}
\coqrun{name=r8}{ssr,test}

The \lstinline/(a,x)$~$== (b,y)/ assumption is reflected to
\lstinline/(a,x)$~$= (b,y)/ by using the \lstinline/eqP/ view
specified by the user.  Here we write \lstinline/==/ to have
all the benefits of a computable function (simplification, reasoning
by cases), but when we need the underlying logical property of
substitutivity we access it via the view \lstinline/eqP/.

\begin{coq}{name=test}{}
Lemma test (x y : nat) : (true,x) == (false,y) -> false.
Proof. by []. Qed.
\end{coq}
\coqrun{name=r7}{ssr,eqtype,eqP}
\coqrun{name=r8}{ssr,test}

This statement is true (or better, the hypothesis is false) by computation.
In this last example the use of \C{==} give us immediate access to
reasoning by cases.

\begin{coq}{name=test}{title=Why one should always use \C{==} (EM)}
Lemma test_EM (x y : nat) : if x == y.+1 then x != 0 else true.
Proof. by case: ifP => // /eqP  ->. Qed.
\end{coq}
\coqrun{name=r7}{ssr,eqtype,eqP}
\coqrun{name=r8}{ssr,test}


\mantra{ Whenever we want to state equality between two expressions, if
	they live in an \C{eqType}, always use \lstinline/==/.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbLEARN{eqType is the base of (abstract) ssr style}
%\mcbREQUIRE{}
%\mcbPROVIDE{\\in}
\section{The generic theory of sequences}\label{sec:seqtypetheory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now that the \lstinline/eqType/ interface equips a type with a well
specified comparison function we can use it to build abstract theories,
for example the one of sequences.

It is worth to remark that the concept of interface is crucial to
the development of such a theory. If we try to develop the theory
of the type \C{(seq T)} for an arbitrary \C{T}, we can't go very far.
For example we can express what belonging to a sequence means, but
not write a program that tests if a value is actually in the list.  As a
consequence we lose the former automation provided by computation
and it also becomes harder to reason by cases on the membership predicate.
On the contrary when we quantify a theory on the type  \C{(seq T)} for a
\C{T} that is an \C{eqType}, we recover all that.  In other words,
by better specifying the types parameters of a generic container,
we define which operations are licit and which properties hold.
So far the only interface we know is \C{eqType}, that is primordial to
boolean reflection.
In the next chapters more elaborate interfaces will enable us to
organize knowledge in articulated ways.

Going back to the abstract theory of sequences over an \C{eqType},
we start by defining the membership operation.

\begin{coq}{name=memseq}{title=Membership}
Section SeqTheory.
Variable T : eqType.
Implicit Type s : seq T.

Fixpoint mem_seq s x :=
  if s is y :: s' then (y == x) || mem_seq s' x else false.
\end{coq}
\coqrun{name=memseq}{ssr,memseq}

Like we did for the overloaded \C{==} notation, we can define the
\C{\\in} (and \C{\\notin}) infix notation.  We can then easily
define what a duplicate-free sequence is, as well as a procedure for
removing duplicates from a sequence. 

\begin{coq}{name=uniq}{}
Fixpoint uniq s :=
  if s is x :: s' then (x \notin s') && uniq s' else true.
Fixpoint undup s :=
  if s is x :: s' then
    if x \in s' then undup s' else x :: undup s'
  else [::].
\end{coq}
\coqrun{name=uniq}{ssr,uniq}

Proofs about such concepts can be made pretty much as if
the type \C{T} was \C{nat} or \C{bool}, i.e. our predicates do
compute.

\begin{coq}{name=undupuniq1}{title=\C{undup} is correct (step 1)}
Lemma in_cons y s x : (x \in y :: s) = (x == y) || (x \in s).
Proof. by []. Qed.

Lemma mem_undup s : undup s =i s.
Proof.
move=> x; elim: s => //= y s IHs.
case Hy: (y \in s); last by rewrite in_cons IHs.
by rewrite in_cons IHs; case: eqP => // ->.
Qed.
\end{coq}
where \C{(A =i B)} is a synonym for \C{(forall x, x \\in A = x \\in B)},

The \C{in\_cons} lemma is just a convenient rewrite rule, while
\C{mem\_undup} says that the \C{undup} function does not drop
any non-duplicate element.  Note that in the proof we use both
the decidability of membership (\C{Hy}) and the decidability of
equality (via \C{eqP}).

\begin{coq}{name=undupuniq2}{title=\C{undup} is correct (step 2)}
Lemma undup_uniq s : uniq (undup s).
Proof.
by elim: s => //= x s IHs; case sx: (x \in s); rewrite //= mem_undup sx.
Qed.
\end{coq}
\coqrun{name=uniqp}{ssr,uniq,undupuniq1,undupuniq2,abort}

The proof of \C{undup\_uniq} requires no new ingredients and
completes the specification of \C{undup}.

The last, very important step in the theory of sequences is to show
that the container preserves the \C{eqType} interface: whenever we can
compare the elements of a sequence, we can also compare sequences.

\begin{coq}{name=seqeqtype}{title=\C{eqType} for sequences}
Fixpoint eqseq s1 s2 {struct s2} :=
  match s1, s2 with
  | [::], [::] => true
  | x1 :: s1', x2 :: s2' => (x1 == x2) && eqseq s1' s2'
  | _, _ => false
  end.

Lemma eqseqP : Equality.axiom eqseq.
Proof.
elim=> [|x1 s1 IHs] [|x2 s2] /=; do? [exact: ReflectT | exact: ReflectF].
case: (x1 =P x2) => [<-|neqx]; last by apply: ReflectF => -[eqx _].
by apply: (iffP (IHs s2)) => [<-|[]].
Qed.

Definition seq_eqMixin := Equality.Mixin eqseqP.
Canonical seq_eqType := @Equality.Pack (seq T) seq_eqMixin.
\end{coq}
\index[coq]{\C{(_ =P _)}}
\index[coq]{\C{(_ =P _)}|seealso {\C{eqP}}}
%\marginnote{cleanup this script}
In this script, \C{(x1 =P x2)} is a notation for \C{(@eqP T x1 x2)},
a proof of the \C{reflect} inductive spec; a case analysis on the term
 \C{(x1 =P x2)} has thus two branches. Since the constructors
\C{ReflectT} and \C{ReflectF} carry a proof, each branch of this
analysis features an extra assumption; the branch corresponding to
\C{ReflectT} has the hypothesis \C{x1 = x2} and the branch
corresponding to \C{ReflectF} has its negation \C{x1 != x2}.

As an example we build a sequence of sequences, and we assert that
we can use the \C{==} and \C{\\in} notation on it, as well as apply
the list operations and theorems on objects of type \C{(seq (seq T))}
when \C{T} is an \C{eqType}.

\begin{coq}{name=itereqtype}{}
Let s1 := [:: 1; 2 ].
Let s2 := [:: 3; 5; 7].
Let ss :  seq (seq nat) := [:: s1 ; s2 ].
Check (ss != [::]) && s1 \in ss.
Check undup_uniq ss.
\end{coq}
\coqrun{name=uniqp2}{ssr,itereqtype}

As we have anticipated in chapter~\ref{ch:prog}, functional programming and lists
can model definite, iterated operations like the ``big'' sum
$\Sigma$.  The next section describes how the generic theory
of iterated operations can be built and made practical thanks again
to programmable type inference.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbLEARN{bigop}
%\mcbREQUIRE{Canonical}
%\mcbPROVIDE{overloaded big notations}
\section{The generic theory of ``big'' operators}\label{sec:bigop}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The objective of the \emph{bigop} library is to provide compact notations
for definite iterated operations and a library of general results about them.

Let us take two examples of iterated operations:

$$
\sum_{i=1}^n f(i) = f(1) + f(2) + \ldots + f(n)
\qquad \bigcup_{a \in A} g(a) = g(a_1) \cup g(a_2) \cup \ldots \cup g(a_{|A|})
$$

To share an infrastructure for this class of operators we have to
identify a common pattern.  First the big symbol in front specifies the
operation being iterated and the neutral element for such operator.
For example if $A$ is empty, then the union of all $g(a)$ is $\emptyset$,
while if $n=0$, then the sum of all $f(i)$ is $0$.
Then the range is an expression identifying a finite set of elements,
sometimes expresses an order (relevant when the iterated operation is not
commutative).  Finally, a general term describes the
items being combined by the iterated operation.

As already mentioned in section~\ref{sec:bigopnat},
the functional programming language
provided by \Coq{} can express in a very natural way iterations over
a finite domain.  In particular such finite domain can be represented
as a list; the general term $f(i)$ by a function \C{(fun i =>
...)}; the operation of evaluating a function on all the
elements of a list and combining the results by the \C{foldr} iterator.

Functional programming can also be used to describe the finite domain.
For example, the list of natural numbers $m, m+1, \ldots, m + (n - m) - 1$
corresponding to the range $m \leq i < n$ can be built using the \C{iota}
function as follows:
\begin{coq}{name=iota}{}
Definition index_iota m n := iota m (n - m).
\end{coq}

The only component of typical notations for iterated operations we have not
discussed yet is the filter, used to iterate the operation only on a
subset of the domain.
For example, to state that the sum of the first $n$ odd numbers is $n^2$,
one could write:
$$
\sum_{i < 2n,\, 2 \nmid i} i = n^2
$$
An alternative way of writing the same summation exploits the general
terms to rule out even numbers:
$$
\sum_{i < n} (i * 2 + 1) = n^2
$$
While this latter writing is elegant, it is harder to support
generically, since the filtering condition is not explicit.
For example the following equation clearly holds for any filter,
range and general term.  It would be hard to express such a statement
if the filter were mixed with the general term, and hence its
negation were not obvious to formulate.
$$
\sum_{i < 2n,\,  2 \mid i} i \;+ \sum_{i < 2n,\,  2 \nmid i}  i = \sum_{i < 2n} i
$$

Lastly, not all filtering conditions can be naturally expressed in the
general term. An example is not being a prime number.

In the light of that, our formal statement concerning the sum
of odd numbers is the following:

\begin{coq}{name=bigop21}{}
Lemma sum_odd n : \sum_(0 <= i < n.*2 | odd i) i = n^2.
\end{coq}
where \C{.*2} is a postfix notation, similar to \C{.+1},
standing for doubling.
Under the hood we expect to find the following expression:

\begin{coq}{name=bigop2}{}
Lemma sum_odd n :
  foldr (fun acc i => if odd i then i + acc else acc)
    0 (index_iota 0 n.*2)
  = n^2
\end{coq}

The following section details how the generic notation for
iterated operation is built and specialized to frequent operations
like $\Sigma$.  Section~\ref{sec:bigoplemmas} focuses on
the generic theory of iterated operations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The generic notation for \C{foldr}}

The generic notation for iterated operations has to be attached to a
slightly specific variant of the \C{foldr} iterator, in order to
clearly identify all components. This variant, called \C{bigop}, 
takes as argument a range \C{(r : seq I)}, a neutral element
\C{(idx : R)}, and a binary operation \C{(op : R -> R -> R)}, to be
iterated. Note that the type of the elements in the range are not the
same as the types of the arguments of the iterated operation. Indeed,
\C{bigop} also uses a function \C{(F : I -> R)}, as well as a
filtering predicate \C{(P : pred I)}, to construct the actual
iteration range. In the end, \C{bigop} filters the range \C{r} using
predicate \C{P}, and precomposes by the function \C{F} before
iterating the binary operation \C{op}. And \C{idx} is the value output
when the filtered range is empty.

\begin{coq}{name=bigop2}{}
Definition bigop R I idx op r (P : pred I) (F : I -> R) : R :=
  foldr (fun i x => if P i then op (F i) x else x) idx r.
\end{coq}

Using the \C{bigop} constant to express our statement leads to

\begin{coq}{name=bigop2}{}
Lemma sum_odd n :
  bigop 0 addn (index_iota 0 n.*2) odd (fun i => i) = n^2.
\end{coq}

Note that \C{odd} is already a predicate on \C{nat}, the general
term is the identity function, the range \C{r} is \C{(index_iota 0
n.*2)}, the iterated operation \C{addn} and the initial value is
\C{0}.

A generic notation can now be attached to \C{bigop}.

\begin{coq}{name=bigopnotation}{}
Notation "\big [ op / idx ]_ ( i <- r | P ) F" :=
  (bigop idx op r (fun i => P%B) (fun i => F)) : big_scope.
\end{coq}
\index[coq]{\C{\\big[../..]_(.."|..) ..}}

Here \C{op} is the iterated operation, \C{idx} the neutral element,
\C{r}, the range, \C{P} the filter (hence the boolean scope)
and \C{F} the general term.  Using such notation the running example
can be stated as follows.

\begin{coq}{name=bigop21}{}
Lemma sum_odd n : \big[addn/0]_(i <- index_iota 0 n.*2 | odd i) i = n^2.
\end{coq}

To obtain a notation closer to the mathematical one, we can specialize
at once the iterated operation and the neutral element as follows.

\begin{coq}{name=bigop3nat}{}
Local Notation "+%N" := addn (at level 0, only parsing).
Notation "\sum_ ( i <- r | P ) F" :=
  (\big[+%N/0%N]_(i <- r | P%B) F%N) : nat_scope.
\end{coq}
\index[coq]{\C{\\sum}}

Such a notation is placed in \C{nat\_scope} as it is specialized
to \C{addn} and \C{0}.  The general term \C{F} is also placed in
the scope of natural numbers.
We can proceed even further and specialize the notation to
a numerical range:

\begin{coq}{name=bigop3nat2}{}
Notation "\big [ op / idx ]_ ( m <= i < n | P ) F" :=
  (bigop idx op (index_iota m n) (fun i : nat => P%B) (fun i => F))
     : big_scope.
Notation "\sum_ ( m <= i < n ) F" :=
  (\big[+%N/0%N]_(m <= i < n) F%N) : nat_scope.
\end{coq}
\coqrun{name=uniqp2}{ssr,bigop,bigop2}

We can now comfortably state the theorem about the sum of odd numbers
inside \C{nat\_scope}.  The proof of this lemma is left as an
exercise; we now focus on a simpler instance, for $n$ equal to $3$, to
introduce the library that equips iterated operations.

\begin{coq}{name=bigop3ex}{}
Lemma sum_odd_3 : \sum_(0 <= i < 3.*2 | odd i) i = 3^2.
Proof.
rewrite unlock /=.
\end{coq}

The \C{bigop} constant is ``locked'' to make the notation steady.  To
unravel its computational behavior one has to rewrite with
the \C{unlock} lemma.

\begin{coqout}{}{}
============================
1 + (3 + (5 + 0)) = 3^2
\end{coqout}

The computation behavior of \C{bigop} is generic; it does not
depend on the iterated operation. By contrast, some results
on iterated operations may depend on a particular property of the
operation. For example, to pull out the last item from the summation,
i.e., using the following lemma


$$
\mbox{if }a \le b\mbox{ then }\sum_{a \le i < b+1} F i = \sum_{a \le i < b} F i + F b
$$

to obtain

\begin{coqout}{}{}
============================
1 + (3 + 0) + 5 = 3^2
\end{coqout}

one really needs the iterated operation, addition here,
to be associative.  Also note
that, given the filter, what one really pulls out is
\C{(if odd 5 then 5 else 0)}, so for the theorem to be true for any
range, \C{0} must also be neutral.

The lemma to pull out the last item of an iterated operation
is provided as the combination of two simpler lemmas called
respectively \C{big\_nat\_recr} and \C{big\_mkcond}.

The former states that one can pull out of an iterated operation on a
numerical range the last element, proviso the range is non-empty.

\begin{coq}{name=bigop3natrec}{}
Lemma big_nat_recr n m F : m <= n ->
  \big[*%M/1]_(m <= i < n.+1) F i = (\big[*%M/1]_(m <= i < n) F i) * F n.
\end{coq}

Such lemma  applies to any operation \C{*\%M} and any neutral element
\C{1} and any generic term \C{F}, while the filter \C{P} is
fixed to \C{true} (i.e., no filter).  The \C{big\_mkcond} lemmas
moves the filter into the generic term.

\begin{coq}{name=bigop3mkcond}{}
Lemma big_mkcond I r (P : pred I) F :
  \big[*%M/1]_(i <- r | P i) F i =
     \big[*%M/1]_(i <- r) (if P i then F i else 1).
\end{coq}

If we chain the two lemmas we can pull out the last item.

\begin{coq-left}{name=bigop3natrec}{}
Lemma sum_odd_3 :
  \sum_(0 <= i < 3.*2 | odd i) i = 3^2.
Proof.
rewrite big_mkcond big_nat_recr //=.
rewrite unlock /=.
\end{coq-left}
\begin{coqout-right}{}{}
============================
\sum_(0 <= i < 5) (if odd i then i else 0)
  + 5 =
  3^2
\end{coqout-right}

When the last item is pulled out, we can unlock the computation
and obtain the following goal:

\begin{coqout}{}{}
============================
0 + (1 + (0 + (3 + (0 + 0)))) + 5 = 3^2
\end{coqout}

It is clear that for the two lemmas to be provable,
one needs the associative property of \C{addn} and also that
\C{0} is neutral.
In other words, the lemmas we used require the operation \C{*\%M} to form
a monoid together with the unit \C{1}.

We detail how this requirement is stated, and automatically satisfied by
\Coq{} in the case of \C{addn}, in the next section.  We conclude this
section by showing that the same lemmas also apply to an iterated
product.

\begin{coq-left}{name=bigop3natrec}{width=5.5cm}
Lemma prod_fact_4 :
  \prod_(1 <= i < 5) i = 4`!.
Proof.
rewrite big_nat_recr //=.
\end{coq-left}
\begin{coqout-right}{}{width=6cm,title=Pulling out the last product}
============================
\prod_(1 <= i < 4) i * 4 = 4`!
$~$
$~$
\end{coqout-right}

This is the reason why we can say that the bigop library is generic:
it works uniformly on any iterated operator, and, provided the operator
has certain properties, it gives uniform access to a palette of lemmas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Assumptions of a bigop lemma}\label{sec:bigoplemmas}

As we anticipated, canonical structures can be indexed not only on
types, but on any term.  In particular we can index them on function
symbols to relate, for example, \C{addn} and its monoid structure.

Here we only present the \C{Monoid} interface that an operation has to
satisfy in order to access a class of generic lemmas.
Chapter~\ref{ch:hierarchy} adds other interfaces to the picture and organizes
the bigop library around them.

\begin{coq}{name=bigop}{}
Module Monoid.
Section Definitions.
Variables (T : Type) (idm : T).

Structure law := Law {
  operator : T -> T -> T;
  _ : associative operator;
  _ : left_id idm operator;
  _ : right_id idm operator
}.
\end{coq}
\coqrun{name=uniqp2}{ssr,bigop}

The \C{Monoid.law} structure relates the \C{operator} (the key used by
canonical structures) to the three properties of monoids.

We can then use this interface as a parameter for a bunch of lemmas,
describing the theory shared by  its instances. The \C{(Monoid.law idx)}
type annotation may seem puzzling at first. It combines various mechanisms
we have seen earlier in the book: once the section \C{Definitions} is
closed, the \C{Variables} \C{(T : Type)} and \C{(idm : T)} are
abstracted in the definition of \C{law} (and the \C{operator}
projection).  Moreover \C{T} becomes an implicit argument: applying
\C{Monoid.law} to \C{idx} thus builds a type.

\begin{coq}{name=bigop1}{}
Coercion operator : law >-> Funclass.
Section MonoidProperties.
Variable R : Type.

Variable idx : R.
Local Notation "1" := idx.

Variable op : Monoid.law idx.
Local Notation "*%M" := op (at level 0).
Local Notation "x * y" := (op x y).
\end{coq}

The lemma we used in the previous section, \C{big\_nat\_recr}, is
stated as follows. Note that
\C{op} is a record, and not a function, but since the \C{operator}
projection is declared as a coercion we can use \C{op} as such.
In particular under the hood of the expression \C{\\big[*\%M/1]} we
find \C{\\big[ operator op / idx ]}.


\begin{coq}{name=bigop1}{}
Lemma big_nat_recr n m F : m <= n ->
  \big[*%M/1]_(m <= i < n.+1) F i = (\big[*%M/1]_(m <= i < n) F i) * F n.
\end{coq}
\coqrun{name=uniqp2}{ssr,bigop}

If we print such a statement once the \C{Section MonoidProperties} is
closed, we see the requirement affecting the operation \C{op} explicitly.

\begin{coqout}{}{}
big_nat_recr :
  forall (R : Type) (idx : R) (op : Monoid.law idx) (n m : nat) (F : nat -> R),
  m <= n ->
    \big[op/idx]_(m <= i < n.+1) F i =
    op (\big[op/idx]_(m <= i < n) F i) (F n)
\end{coqout}

Note that wherever the operation \C{op} occurs, we also find
the \C{Monoid.operator} projection.

To make this lemma available for addition on natural numbers,
we need to declare the canonical monoid structure on \C{addn}.

\begin{coq}{name=natmonoid}{}
Canonical addn_monoid := Monoid.Law addnA add0n addn0.
\end{coq}

This command adds the following rule to the canonical structures
index:

\vspace{1ex}
\noindent
\begin{tcolorbox}[colframe=orange!60!white,before=\hfill,after=\hfill,width=8cm,center title,tabularx={ll|l},fonttitle=\sffamily\bfseries,title=canonical structures Index]
projection & value & solution \\ \hline
\lstinline/Monoid.operator/ & \lstinline/addn/ & \lstinline/addn_monoid/  \\
\hline
\end{tcolorbox}

Whenever the lemma is applied to an expression about natural numbers
as

\begin{coq}{name=bigop2}{}
Lemma test : \sum_(0 <= i < 6) i =  \sum_(0 <= i < 5) i + 5.
Proof. by apply: big_nat_recr. Qed.
\end{coq}

the following unification problem has to be solved:
\C{addn} versus \C{(operator ?m)}.
Inferring a value for \C{?m} means
inferring a proof that \C{addn} forms a monoid with \C{0}; this is a
prerequisite for the \C{big\_nat\_recr} lemma we don't have to provide
by hand.

% The problem \C{i} versus \C{(?F i)}
% is a hard problem too, since \C{?F} is a program, but it has
% a unique solution: being \C{i} a locally bound variable, not in the
% scope of the body of \C{?F}, the only way for \C{(?F i)} to
% produce \C{i} is to return its input.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Searching the bigop library}

Searching the bigop library for a lemma is slightly harder than
searching the other libraries as explained in section~\ref{sec:search}.
In particular one can hardly search with patterns.  For example
the following search returns no results:

\begin{coq}{}{}
Search _ (\sum_(0 <= i < 0) _).
\end{coq}
A lemma stating that an empty sum is zero is not part of the library.
What is part of the library is a lemma that says that, if the list
being folded is \C{nil}, then the result is the initial value.  Such
a lemma, called \C{big_nil}, thus mentions only \C{[::]} in its
statement, and not the (logically) equivalent \C{(index_iota 0 0)}.
Still the goal \C{(\\sum_(0 <= i < 0) i = 0)} can be solved by
\C{big_nil}.  Finally, the pattern we provide specifies a trivial
filter, while the lemma is true for any filtering predicate.
Of course one can craft a pattern that finds such lemma, but it is very
verbose and hence inconvenient.

\begin{coq}{}{}
Search _ (\big[_/_]_(i <- [::] | _) _).
\end{coq}

The recommended way to search the library is by name, using the word
``big''.  For example to find all lemmas allowing one to prove the
equality of two iterated operators one can \C{ Search "eq" "big"}.
Similarly, induction lemmas can be found with \C{ Search "ind"
"big"};  index exchange lemmas with \C{ Search "exchange" "big"};
lemmas pulling out elements from the iteration
with \C{ Search "rec" "big"}; lemmas  working on the filter condition
with \C{ Search "cond" "big"}, and so on.


Finally the \mcbMC{} user is advised to read the contents of the
\lib{bigop} file in order to get acquainted with the naming policy used
in that library.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbLEARN{Real bigop notation}
%\mcbREQUIRE{bigop}
%\mcbPROVIDE{BigBody}
\section{Stable notations for big operators}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\warnverytechnical{}

The \C{bigop} constant and the notations attached to it
are defined in a more complex way in the \mcbMC{} library.
In particular, \C{bigop} is fragile because the predicate and
the general term do not share the same binder.  For example,
if we write the following

\begin{coq}{name=bigop2}{}
Lemma sum_odd n :
  bigop 0 addn (index_iota 0 n.*2) (fun j => odd j) (fun i => i) = n^2
\end{coq}

What should be printed by the system?  It is an iterated sum on
\C{j} or \C{i}?  Similarly, if the index becomes unused during a
proof, which name should be printed?

\begin{coq}{name=bigop2}{}
Lemma sum_0 n :
  bigop 0 addn (index_iota 0 n) (fun _ => true) (fun _ => 0) = 0
\end{coq}

To solve these problems we craft a box, \C{BigBody}, with
separate compartments for each sub component.  Such a box will
be used under a single binder and will hold an occurrence of the
bound variable even if it is unused in the predicate and in the
general term.

\begin{coq}{name=bigop2}{}
Inductive bigbody R I := BigBody of I & (R -> R -> R) & bool & R.
\end{coq}

The arguments of \C{BigBody} are respectively the index, the
iterated operation, the filter and the generic expression.
For our running example the \C{bigbody} component would be:

\begin{coq}{name=bigopeven}{}
Definition sum_odd_def_body i := BigBody i addn (odd i) i.
\end{coq}

It is then easy to turn such compound term into the function expected
by \C{foldr}:

\begin{coq}{name=bigopapplybig}{}
Definition applybig {R I} (body : bigbody R I) acc :=
  let: BigBody _ op b v := body in if b then op v acc else acc.
\end{coq}

Finally the generic iterated operator can be defined as follows.

\begin{coq}{name=bigop3}{}
Definition bigop R I idx r (body : I -> bigbody R I) :=
  foldr (applybig \o body) idx r.
\end{coq}

And a generic notation can be attached to it.

\begin{coq}{name=bigopnotation}{}
Notation "\big [ op / idx ]_ ( i <- r | P ) F" :=
  (bigop idx r (fun i => BigBody i op P%B F)) : big_scope.
\end{coq}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbLEARN{Declaring overloaded notations}
%\mcbREQUIRE{Canonical}
%\mcbPROVIDE{Notation}
\section{Working with overloaded notations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\warntechnical{}

This little section deals with two ``technological issues'' the reader
may need to know in order to define overloaded notations or work
comfortably with them.

The first one is the necessity to tune the behavior of the
simplification tactic (the \C{/=} switch) to avoid losing the
head constant to which the overloaded notation is attached.
For example the following term:

\begin{coq}{name=infix,print}{}
  (@eq_op bool_eqType true false).
\end{coq}
can be simplified (reduced) to the following one

\begin{coq}{name=infix,print}{}
  (@eqb true false).
\end{coq}

While the two terms are logically equivalent (i.e., the logic cannot
distinguish them), the pretty printer can.  The overloaded
\C{==} notation is attached to the \C{eq\_op} constant, and if such
constant fades away the notation follows it.  \Coq{} lets one declare
constants that should not be automatically simplified away, unless
they occur in a context that demands it.

\begin{coq}{name=infix,print}{}
Arguments eq_op {_} _ _ : simpl never.
Eval simpl in forall x y : bool, x == y.
Eval simpl in forall x y : bool, true == false || x == y.
\end{coq}

The first call to \C{simpl} does not reduce away \C{eq\_op}
leaving the expression untouched.  In the second example, it
does reduce to \C{false} the test \C{(true == false)} in order to
simplify the \C{||} connective.

The converse technological issue may arise when canonical structure
inference ``promotes'' the operator name to a projection of the
corresponding canonical monoid structure.

\begin{coq-left}{name=infix,print}{width=5.3cm}
Implicit Type l : seq nat.
Lemma example F l1 l2 :
 \sum_(i <- l1 ++ l2) F i =
 \sum_(i <- l2 ++ l1) F i.
Proof.
rewrite big_cat.
$~$
\end{coq-left}
\begin{coqout-right}{run=roar}{title=Response after line 6,width=6.9cm}
F : nat -> nat
l1, l2 : seq nat
============================
addn_monoid
 (\big[addn_monoid/0]_(i <- l1) F i)
 (\big[addn_monoid/0]_(i <- l2) F i) =
\sum_(i <- (l2 ++ l1)) F i
\end{coqout-right}

It is not uncommon to see \C{/=} switch in purely algebraic proofs
(where no computation is really involved) just to clean up the display
of the current conjecture.
The proof concludes as follows:

\begin{coq}
rewrite /=.
by rewrite addnC -big_cat.
Qed.
\end{coq}



% TODO : Discuss also the dangers of breaking abstraction barriers, as a
% good practice.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mcbLEARN{Implement \lstinline/[foo of nat]/}
%\mcbREQUIRE{Canonical}
%\mcbPROVIDE{Phantom}

\section{Ad-hoc polymorphism}

%\warnverytechnical{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
\subsection{Phantom types}



First of all, canonical structure resolution kicks in during
unification that in turn is used to compare types.  Types are
compared whenever a function is applied to an argument, and in
particular the type expected by the function and the one of the
argument are unified.  What we need to craft is a mechanism that
takes any input term (proper terms like \C{addn} but also types as \C{nat})
and puts it into a type.  We will then wire things up so that such
type is unified with another one containing the application of
a projection to an unknown canonical structure instance.

\begin{coq}{name=phantom}{}
Inductive phantom (T : Type) (p : T) := Phantom.
\end{coq}

The \C{Phantom} constructor expects two arguments.  If we apply
it to \C{nat}, as in \C{(Phantom Type nat)}, we obtain a term
of type \C{(phantom Type nat)}.  If we apply it to \C{addn} as
in \C{(Phantom (nat -> nat -> nat) addn)} we obtain a term
of type \C{(phantom (nat -> nat -> nat) addn)}.  In both cases the
input term (\C{nat} and \C{addn} respectively) is now part of a type.

The following example defines a notation \C{\{set T\}} that
fails if \C{T} is not an \C{eqType}: it is an alias of the type
\C{(seq T)} that imposes extra requirements on the type argument.

\begin{coq}{name=set}{}
Definition set_of (T : eqType) (_ : phantom Type (Equality.sort T)) := seq T.
Notation "{ 'set' T }" := (set_of _ (Phantom Type T))
  (at level 0, format "{ 'set'  T }") : type_scope.
\end{coq}

When type inference runs on \C{\{set nat\}} the underlying term being
typed is \C{(set\_of ?T (Phantom Type nat))}.
The unification problem arising for the last argument of \C{set\_of}
is \C{(phantom Type (Equality.sort ?T))} versus
\C{(phantom Type nat)}, that in turn contains the sub problem
we are interested in: \C{(Equality.sort ?T)} versus \C{nat}.

\subsection{Querying canonical structures}\label{sec:phantom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\warntechnical{}

It is possible to ask \Coq{} whether a certain term validates an
interface.  For example, to check if \C{addn} forms a monoid one can
\C{Check [law of addn]}.  A notation of this kind exists for any
interface, for example \C{[eqType of nat]} is another valid query to
check if \C{nat} is equipped with a canonical comparison function.

This mechanism can also be used to craft notations that assert
if one of their arguments validates an interface.  For example
imagine one wants to define the concept of finite set as an
alias of \C{(seq T)} but such that only values for \C{T} being
\C{eqType}s are accepted.

The rest of this section introduces the general mechanism of phantom
types used to trigger canonical structure resolution.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Exercises}
% 
% %%%%%%%%%%
% \begin{Exercise}[label=ex:sumoddsquare,difficulty=0,title={Sum of $2n$ odd numbers}]
% Show the following lemma using the theory of big operators
% 
% \begin{coq}{name=bigop222}{}
% Lemma sum_odd n : \sum_(0 <= i < n.*2 | odd i) i = n^2.
% \end{coq}
% \end{Exercise}
% 
% \subsection{Solutions}
% 
% %%%%%%%%%
% \begin{Answer}[ref=ex:sumoddsquare]
% \begin{coq}{name=bigop222}{}
% Lemma sum_odd n : \sum_(0 <= i < n.*2 | odd i) i = n^2.
% Proof.
% elim: n => [|n IHn]; first by rewrite unlock.
% rewrite doubleS big_mkcond 2?big_nat_recr // -big_mkcond /=.
% by rewrite {}IHn odd_double /= addn0 -addnn -!mulnn; ring.
% Qed.
% \end{coq}
% \end{Answer}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \mcbLEARN{difference with type classes, limitations}
% \mcbREQUIRE{RecCanonical,Coercions}
% \mcbPROVIDE{}
% \mcbsection{Discussion about type inference}\label{sec:typeinfrelated}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% NOT sure I want to keep that discussion.
%
% The points I want to make are:
% \begin{itemize}
% \item during type inference, HO infers (morally) the minimum
% 	info necessary to make things well typed (if we were 1st order
% 	that would really be true).
% 	hence we KNOW when CS inference is triggered EXACTLY.
% 	this is a big difference w.r.t. type classes. It is like a
% 	Prolog program where goals are reordered randomly
% \item I also want to talk about the overlapping instances problem,
% 	that is ``easy'' with TC, hard with CS and envisage an
% 	extension.
% \item limitation of coercions, from both the usability perspective and
% 	the expressive power they offer
% \end{itemize}
